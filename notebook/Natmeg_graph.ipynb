{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import os\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.contrib import tzip\n",
    "from tqdm.contrib.itertools import product\n",
    "import itertools\n",
    "import pickle\n",
    "####\n",
    "import mne\n",
    "# from mne.preprocessing import create_ecg_epochs, create_eog_epochs\n",
    "# from mne.coreg import Coregistration\n",
    "# from mne.minimum_norm import make_inverse_operator, apply_inverse_raw\n",
    "import learn_graph\n",
    "# from mne_connectivity import spectral_connectivity_epochs, spectral_connectivity_time\n",
    "####\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "# import nilearn\n",
    "from nilearn.plotting import view_connectome, plot_connectome\n",
    "####\n",
    "# import torch\n",
    "# from umap.umap_ import UMAP\n",
    "# from sklearn.cluster import DBSCAN, KMeans\n",
    "# from matplotlib.animation import FuncAnimation\n",
    "####\n",
    "from scipy.stats import ttest_ind, mannwhitneyu, shapiro\n",
    "import statsmodels.api as sm\n",
    "# from sklearn.mixture import GaussianMixture\n",
    "# from sklearn.decomposition import PCA\n",
    "####\n",
    "# from tensorpac import Pac\n",
    "from scipy.signal import coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of subjects with their files\n",
    "directory = '/Users/payamsadeghishabestari/KI_MEG/KI_RS/tinmeg3' \n",
    "folders_dict = {}\n",
    "for folder in sorted(os.listdir(directory)): ## iterate over folders in that directory\n",
    "    f = os.path.join(directory, folder)\n",
    "    if os.path.isdir(f): ## select only folders\n",
    "        folders_dict[folder] = f\n",
    "\n",
    "raw_rs_dict = {}\n",
    "raw_er_dict = {}\n",
    "subjects = list(folders_dict.keys())\n",
    "folders = list(folders_dict.values())\n",
    "for subject_id, folder in zip(subjects, folders):\n",
    "    raw_er_dict[subject_id] = []\n",
    "    files = sorted(os.listdir(folder))\n",
    "    if '.DS_Store' in files:\n",
    "        files.remove('.DS_Store')\n",
    "    for file in files:\n",
    "        if 'empty_room_before' in file:\n",
    "            raw_er_dict[subject_id].append(os.path.join(folder, files[1]))\n",
    "        if 'empty_room_after' in file:\n",
    "            raw_er_dict[subject_id].append(os.path.join(folder, files[0]))\n",
    "    f_rs = os.path.join(folder, files[-1])\n",
    "    raw_rs_dict[subject_id] = f_rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing and processing of recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting up parameters\n",
    "verbose = False\n",
    "sfreq = 250\n",
    "(l_freq, h_freq) = (0.1, 80)\n",
    "(l_freq_delta, h_freq_delta) = (0.5, 4)\n",
    "(l_freq_theta, h_freq_theta) = (4, 8)\n",
    "(l_freq_alpha, h_freq_alpha) = (8, 13)\n",
    "(l_freq_beta, h_freq_beta) = (13, 30)\n",
    "(l_freq_gamma, h_freq_gamma) = (30, 45)\n",
    "subjects_dir = '/Applications/freesurfer/7.4.1/subjects'\n",
    "fname_fsaverage_src = '/Users/payamsadeghishabestari/mne_data/MNE-fsaverage-data/fsaverage/bem/fsaverage-ico-5-src.fif'\n",
    "src_to = mne.read_source_spaces(fname_fsaverage_src, verbose=False)\n",
    "method = \"dSPM\"\n",
    "snr = 3.0\n",
    "lambda2 = 1.0 / snr**2\n",
    "atlases = ['aparc', 'aparc.a2009s']\n",
    "modes = ['mean', 'mean_flip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in tqdm(subjects[16:]): \n",
    "    \n",
    "    ## reading the resting state and empty room recordings and computing noise covariance\n",
    "    start_time = time.time()\n",
    "    print(f'reading the MEG file of subejct {subject} ...')\n",
    "    fname = raw_rs_dict[subject]\n",
    "    raw = mne.io.read_raw_fif(fname=fname, preload=True, allow_maxshield=True, verbose=verbose)\n",
    "    raw_er_fnames = raw_er_dict[subject]\n",
    "    print(f'reading and computing noise covariance of the subject {subject} ...')\n",
    "    if len(raw_er_fnames) == 0:\n",
    "        noise_cov = mne.make_ad_hoc_cov(info=raw.info, std=None, verbose=verbose) # 5 fT/cm, 20 fT\n",
    "    if len(raw_er_fnames) == 1:\n",
    "        raw_er = mne.io.read_raw_fif(fname=raw_er_fnames[0], preload=True,\n",
    "                                    allow_maxshield=True, verbose=verbose)\n",
    "        noise_cov = mne.compute_raw_covariance(raw=raw_er, method='empirical', verbose=verbose)\n",
    "    if len(raw_er_fnames) == 2:   \n",
    "        raw_er_1 = mne.io.read_raw_fif(fname=raw_er_fnames[0], preload=True,\n",
    "                                    allow_maxshield=True, verbose=verbose)\n",
    "        raw_er_2 = mne.io.read_raw_fif(fname=raw_er_fnames[1], preload=True,\n",
    "                                    allow_maxshield=True, verbose=verbose)\n",
    "        raw_er = mne.concatenate_raws(raws=[raw_er_1, raw_er_2], verbose=verbose)\n",
    "        noise_cov = mne.compute_raw_covariance(raw=raw_er, method='empirical', verbose=verbose)    \n",
    "    \n",
    "    # ###### Sensor Space ######\n",
    "\n",
    "    ## resampling and filtering the data\n",
    "    print('resampling and filtering the data, be patient, will last a while ...')\n",
    "    raw.resample(sfreq=sfreq, verbose=verbose)\n",
    "    raw.filter(l_freq=l_freq, h_freq=h_freq, verbose=verbose) \n",
    "\n",
    "    ## creating ECG and EOG evoked responses\n",
    "    ecg_evoked_meg,  ecg_evoked_grad = create_ecg_epochs(raw,\n",
    "                                    verbose=verbose).average().apply_baseline(baseline=(None, -0.2),\n",
    "                                    verbose=verbose).plot_joint(picks=['meg', 'grad'], show=False)\n",
    "    eog_evoked_meg,  eog_evoked_grad = create_eog_epochs(raw,\n",
    "                                    verbose=verbose).average().apply_baseline(baseline=(None, -0.2),\n",
    "                                    verbose=verbose).plot_joint(picks=['meg', 'grad'], show=False)\n",
    "\n",
    "    ## computing ICA and remove ECG, EOG and muscle artifacts (if any) and interpolating (if any)\n",
    "    print('computing ICA (this might take a while) ...')\n",
    "    ica = mne.preprocessing.ICA(n_components=0.95, max_iter=800, method='infomax',\n",
    "                                random_state=42, fit_params=dict(extended=True)) \n",
    "    ica.fit(raw, verbose=verbose) \n",
    "    ecg_indices, ecg_scores = ica.find_bads_ecg(raw, method=\"ctps\", measure='zscore', verbose=verbose)\n",
    "    if len(ecg_indices) > 0:\n",
    "        ecg_component = ica.plot_properties(raw, picks=ecg_indices, verbose=verbose, show=False)\n",
    "    emg_indices, emg_scores = ica.find_bads_muscle(raw, verbose=verbose)\n",
    "    if len(emg_indices) > 0:\n",
    "        emg_component = ica.plot_properties(raw, picks=emg_indices, verbose=verbose, show=False)\n",
    "    eog_indices, eog_scores = ica.find_bads_eog(raw, ch_name='EOG002', verbose=verbose) \n",
    "    if len(eog_indices) > 0:\n",
    "        eog_component = ica.plot_properties(raw, picks=eog_indices, verbose=verbose, show=False)\n",
    "    saccade_indices, saccade_scores = ica.find_bads_eog(raw, ch_name='EOG001', verbose=verbose) \n",
    "    if len(saccade_indices) > 0:\n",
    "        saccade_component = ica.plot_properties(raw, picks=saccade_indices, verbose=verbose, show=False)\n",
    "\n",
    "    exclude_idxs = ecg_indices + emg_indices + eog_indices + saccade_indices\n",
    "    ica.apply(raw, exclude=exclude_idxs, verbose=verbose)\n",
    "    raw.interpolate_bads(verbose=verbose)\n",
    "\n",
    "    ## bandpassing the recording into alpha and gamma bands and saving them\n",
    "    print('Bandpassing data ...')\n",
    "    raw_delta = raw.copy().filter(l_freq=l_freq_delta, h_freq=h_freq_delta, verbose=verbose)\n",
    "    raw_theta = raw.copy().filter(l_freq=l_freq_theta, h_freq=h_freq_theta, verbose=verbose)\n",
    "    raw_beta = raw.copy().filter(l_freq=l_freq_beta, h_freq=h_freq_beta, verbose=verbose)\n",
    "\n",
    "    fname_delta = f'/Users/payamsadeghishabestari/meg_gsp/raws/tinmeg3/delta/{subject}_raw_tsss.fif'\n",
    "    fname_theta = f'/Users/payamsadeghishabestari/meg_gsp/raws/tinmeg3/theta/{subject}_raw_tsss.fif'\n",
    "    fname_beta = f'/Users/payamsadeghishabestari/meg_gsp/raws/tinmeg3/beta/{subject}_raw_tsss.fif'\n",
    "    raw_delta.save(fname=fname_delta, overwrite=True, verbose=verbose)\n",
    "    raw_theta.save(fname=fname_theta, overwrite=True, verbose=verbose)\n",
    "    raw_beta.save(fname=fname_beta, overwrite=True, verbose=verbose)\n",
    "\n",
    "\n",
    "    # fname_alpha = f'/Users/payamsadeghishabestari/meg_gsp/raws/tinmeg3/alpha/{subject}_raw_tsss.fif'\n",
    "    # fname_gamma = f'/Users/payamsadeghishabestari/meg_gsp/raws/tinmeg3/gamma/{subject}_raw_tsss.fif' \n",
    "    # raw_alpha = mne.io.read_raw(fname=fname_alpha, preload=True, verbose=False)\n",
    "    # raw_gamma = mne.io.read_raw(fname=fname_gamma, preload=True, verbose=False)\n",
    "    \n",
    "    # fname_delta = f'/Users/payamsadeghishabestari/meg_gsp/raws/tinmeg1/delta/{subject}_raw_tsss.fif'\n",
    "    # fname_theta = f'/Users/payamsadeghishabestari/meg_gsp/raws/tinmeg1/theta/{subject}_raw_tsss.fif'\n",
    "    # fname_beta = f'/Users/payamsadeghishabestari/meg_gsp/raws/tinmeg1/beta/{subject}_raw_tsss.fif'\n",
    "    # raw_delta = mne.io.read_raw(fname=fname_delta, preload=True, verbose=False)\n",
    "    # raw_theta = mne.io.read_raw(fname=fname_theta, preload=True, verbose=False)\n",
    "    # raw_beta = mne.io.read_raw(fname=fname_beta, preload=True, verbose=False)\n",
    "\n",
    "    ###### Source Space ######\n",
    "    spacing = \"oct6\"\n",
    "    if subject == \"1004\":\n",
    "        spacing = \"oct4\"\n",
    "    if subject == \"1045\":\n",
    "        spacing = \"ico5\"\n",
    "    \n",
    "    ## setting up the surface source space\n",
    "    print(f'Setting up bilateral hemisphere surface-based source space with subsampling for subject {subject} ...')\n",
    "    src = mne.setup_source_space(subject=f'{subject}', spacing=spacing,\n",
    "                                subjects_dir=subjects_dir, n_jobs=-1, verbose=True)\n",
    "\n",
    "    ## setting up the boundary-element model (BEM) \n",
    "    print(f'Creating a BEM model for the subject ...')\n",
    "    bem_model = mne.make_bem_model(subject=f'{subject}', ico=4, subjects_dir=subjects_dir, verbose=True)  \n",
    "    bem = mne.make_bem_solution(bem_model, verbose=verbose)\n",
    "    \n",
    "    ## aligning coordinate frame (coregistration MEG-MRI)\n",
    "    print(f'Coregistering MRI with a subjects head shape ...')\n",
    "    coreg = Coregistration(raw_delta.info, f'{subject}', subjects_dir, fiducials='auto')\n",
    "    coreg.fit_fiducials(verbose=verbose)\n",
    "    coreg.fit_icp(n_iterations=40, nasion_weight=2.0, verbose=verbose) \n",
    "    coreg.omit_head_shape_points(distance=5.0 / 1000)\n",
    "    coreg.fit_icp(n_iterations=40, nasion_weight=10, verbose=verbose) \n",
    "    fname_trans = f'/Users/payamsadeghishabestari/meg_gsp/trans/{subject}-trans.fif'\n",
    "    mne.write_trans(fname_trans, coreg.trans, overwrite=True, verbose=verbose)\n",
    "    \n",
    "    ## computing the forward solution\n",
    "    print(f'Computing the forward solution ...')\n",
    "    fwd = mne.make_forward_solution(raw_delta.info, trans=coreg.trans, src=src, bem=bem, meg=True,\n",
    "                                    eeg=False, mindist=5.0, n_jobs=None, verbose=verbose)\n",
    "    \n",
    "    # ## computing the minimum-norm inverse solution\n",
    "    print(f'Computing the minimum-norm inverse solution ...')\n",
    "    inverse_operator = make_inverse_operator(raw_delta.info, fwd, noise_cov, loose=0.2, depth=0.8, verbose=verbose)\n",
    "\n",
    "    ## compute source estimate object\n",
    "    print(f'Computing the source estimate object ...')\n",
    "    for raw_bp, title_raw in zip([raw_delta, raw_theta, raw_beta], ['delta', 'theta', 'beta']):\n",
    "\n",
    "        stc = apply_inverse_raw(raw_bp, inverse_operator, lambda2, method=method, verbose=verbose)\n",
    "\n",
    "        # ## morphing to template brain\n",
    "        # morph = mne.compute_source_morph(stc, subject_from=f'0{subject}', subject_to=\"fsaverage\",\n",
    "        #                                 subjects_dir=subjects_dir, src_to=src_to)\n",
    "        # stc_morph = morph.apply(stc)\n",
    "    \n",
    "        ## from source estimate object to brain parcels\n",
    "        for atlas, title_atlas in zip(atlases, ['aparc', 'aparc.a2009s']):\n",
    "            if atlas == 'aparc':\n",
    "                labels = mne.read_labels_from_annot(subject=f'{subject}', parc=atlas,\n",
    "                                                    subjects_dir=subjects_dir, verbose=verbose)\n",
    "            if atlas == 'aparc.a2009s':\n",
    "                labels = mne.read_labels_from_annot(subject=f'{subject}', parc=atlas,\n",
    "                                                    subjects_dir=subjects_dir, verbose=verbose)[:-2]\n",
    "\n",
    "            for mode in modes:\n",
    "                tcs = stc.extract_label_time_course(labels, src, mode=mode, allow_empty=True, verbose=verbose)\n",
    "                file_name = f'/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg3/subject_{subject}_{title_raw}_{title_atlas}_{mode}.npy'\n",
    "                np.save(file=file_name, arr=tcs, allow_pickle=True)\n",
    "\n",
    "    # # creating a report for each subjects\n",
    "    # report = mne.Report(title=f'report_subject_{subject}', verbose=verbose)\n",
    "\n",
    "    # report.add_raw(raw=raw, title='recording after preprocessing', butterfly=False, psd=False) \n",
    "    # report.add_figure(fig=ecg_evoked_meg, title='ECG evoked MEG', image_format='PNG')\n",
    "    # report.add_figure(fig=ecg_evoked_grad, title='ECG evoked Gradiometer', image_format='PNG')\n",
    "    # report.add_figure(fig=eog_evoked_meg, title='EOG evoked MEG', image_format='PNG')\n",
    "    # report.add_figure(fig=eog_evoked_grad, title='EOG evoked Gradiometer', image_format='PNG')\n",
    "\n",
    "    # if len(ecg_indices) > 0:\n",
    "    #     report.add_figure(fig=ecg_component, title='ECG component', image_format='PNG')\n",
    "    # if len(emg_indices) > 0:\n",
    "    #     report.add_figure(fig=emg_component, title='EMG component', image_format='PNG')\n",
    "    # if len(eog_indices) > 0:\n",
    "    #     report.add_figure(fig=eog_component, title='EOG component (blink)', image_format='PNG')  \n",
    "    # if len(saccade_indices) > 0:\n",
    "    #     report.add_figure(fig=saccade_component, title='EOG component (saccade)', image_format='PNG') \n",
    "\n",
    "    # report.add_bem(subject=f'{subject}', subjects_dir=subjects_dir, title=\"MRI & BEM\", decim=10, width=512)\n",
    "    # report.add_trans(trans=fname_trans, info=raw.info, subject=f'{subject}',\n",
    "    #                 subjects_dir=subjects_dir, alpha=1.0, title=\"Co-registration\")\n",
    "    \n",
    "    # fname_report = f'/Users/payamsadeghishabestari/meg_gsp/reports/report_subject_{subject}.html'\n",
    "    # report.save(fname=fname_report, open_browser=False, overwrite=True, verbose=verbose)\n",
    "    # print(f'elapsed time for subject {subject} was {time.time() - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## only for subject 1045\n",
    "subject = \"1045\"\n",
    "raw_delta = mne.io.read_raw_fif(fname=\"/Users/payamsadeghishabestari/meg_gsp/raws/tinmeg3/delta/1045_raw_tsss.fif\", preload=True, verbose=False)\n",
    "raw_theta = mne.io.read_raw_fif(fname=\"/Users/payamsadeghishabestari/meg_gsp/raws/tinmeg3/theta/1045_raw_tsss.fif\", preload=True, verbose=False)\n",
    "raw_beta = mne.io.read_raw_fif(fname=\"/Users/payamsadeghishabestari/meg_gsp/raws/tinmeg3/beta/1045_raw_tsss.fif\", preload=True, verbose=False)\n",
    "\n",
    "inverse_operator = mne.minimum_norm.read_inverse_operator(fname=\"/Users/payamsadeghishabestari/meg_gsp/stc_labels/1045-inv.fif\", verbose=False)\n",
    "src = mne.read_source_spaces(fname=\"/Users/payamsadeghishabestari/meg_gsp/stc_labels/1045-src.fif\", verbose=False)\n",
    "\n",
    "start, stop = raw_delta.time_as_index([100, 200])\n",
    "\n",
    "for raw, raw_title in zip([raw_delta, raw_theta, raw_beta], [\"delta\", \"theta\", \"beta\"]): \n",
    "    stc = apply_inverse_raw(raw, inverse_operator, lambda2, method=method, start=start, stop=stop, verbose=True)\n",
    "    \n",
    "    for atlas, title_atlas in zip(atlases, ['aparc', 'aparc.a2009s']):\n",
    "        if atlas == 'aparc':\n",
    "            labels = mne.read_labels_from_annot(subject=f'{subject}', parc=atlas,\n",
    "                                                subjects_dir=subjects_dir, verbose=True)\n",
    "        if atlas == 'aparc.a2009s':\n",
    "            labels = mne.read_labels_from_annot(subject=f'{subject}', parc=atlas,\n",
    "                                                subjects_dir=subjects_dir, verbose=True)[:-2]\n",
    "        for mode in modes:\n",
    "            tcs = stc.extract_label_time_course(labels, src, mode=mode, allow_empty=True, verbose=verbose)\n",
    "            file_name = f'/Users/payamsadeghishabestari/meg_gsp/stc_labels/subject_{subject}_{raw_title}_{title_atlas}_{mode}_2.npy'\n",
    "            np.save(file=file_name, arr=tcs, allow_pickle=True)\n",
    "\n",
    "\n",
    "for raw_title, title_atlas, mode in product([\"delta\", \"theta\", \"beta\"], ['aparc', 'aparc.a2009s'], ['mean', 'mean_flip']):\n",
    "    arrs_list = []\n",
    "    for i in range(1, 4):\n",
    "        file_name = f'/Users/payamsadeghishabestari/meg_gsp/stc_labels/subject_{subject}_{raw_title}_{title_atlas}_{mode}_{i}.npy'\n",
    "        arrs_list.append(np.load(file=file_name, allow_pickle=True))\n",
    "\n",
    "    arr = np.concatenate(arrs_list, axis=1)\n",
    "    np.save(f'/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg3/subject_{subject}_{raw_title}_{title_atlas}_{mode}.npy', arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of one single subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## learning graph\n",
    "fname = '/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg1/subject_539_alpha_aparc_mean.npy'\n",
    "stc_in_labels = np.load(file=fname, allow_pickle=True)\n",
    "graph_matrices = {}\n",
    "\n",
    "graph_matrix = learn_graph.log_degree_barrier(X=stc_in_labels, dist_type='sqeuclidean', alpha=1,\n",
    "                                            beta=1, step=0.5, w0=None, maxit=10000, rtol=1e-16,\n",
    "                                            retall=False, verbosity='LOW')\n",
    "\n",
    "graph_matrix = learn_graph.l2_degree_reg(X=stc_in_labels, dist_type='sqeuclidean', alpha=1,\n",
    "                                        s=None, step=0.5, w0=None, maxit=10000, rtol=1e-16,\n",
    "                                        retall=False, verbosity='LOW')\n",
    "\n",
    "### extracting adjacency matrix\n",
    "subject = 539\n",
    "atlas = 'aparc'\n",
    "subjects_dir = '/Applications/freesurfer/7.4.1/subjects'\n",
    "\n",
    "if atlas == 'aparc':\n",
    "    labels = mne.read_labels_from_annot(subject=\"fsaverage\", parc=atlas,\n",
    "                                        subjects_dir=subjects_dir, verbose=False)[:-1]\n",
    "if atlas == 'aparc.a2009s':\n",
    "    labels = mne.read_labels_from_annot(subject=\"fsaverage\", parc=atlas,\n",
    "                                        subjects_dir=subjects_dir, verbose=False)[:-2]\n",
    "node_coords = []\n",
    "for label in labels:\n",
    "    if label.hemi == 'lh':\n",
    "        hemi = 0\n",
    "    if label.hemi == 'rh':\n",
    "        hemi = 1\n",
    "    center_vertex = label.center_of_mass(subject=\"fsaverage\", \n",
    "                                        restrict_vertices=False, \n",
    "                                        subjects_dir=None)\n",
    "    mni_pos = mne.vertex_to_mni(center_vertex, hemis=hemi,\n",
    "                            subject=\"fsaverage\", subjects_dir=None)\n",
    "    node_coords.append(mni_pos)\n",
    "\n",
    "node_coords = np.array(node_coords)\n",
    "ticks = [lb.name for lb in labels]\n",
    "\n",
    "### visualizing\n",
    "corr_matrix = np.corrcoef(graph_matrix, rowvar=False)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "sns.heatmap(graph_matrix, cmap=\"hot\", annot=False, xticklabels=ticks, yticklabels=ticks)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(11, 5))\n",
    "plot_connectome(adjacency_matrix=graph_matrix, node_coords=node_coords,\n",
    "                node_color='k', node_size=20, axes=axes,\n",
    "                edge_threshold='90%')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot it for all subjects\n",
    "subjects_dir = '/Applications/freesurfer/7.4.1/subjects'\n",
    "dir_control = '/Users/payamsadeghishabestari/KI_MEG/KI_RS/tinmeg1' \n",
    "dir_case = '/Users/payamsadeghishabestari/KI_MEG/KI_RS/tinmeg3' \n",
    "subjects_control = sorted(os.listdir(dir_control))[1:]\n",
    "subjects_case = sorted(os.listdir(dir_case))[1:]\n",
    "subjects = subjects_control + subjects_case\n",
    "freqs = [\"alpha\", \"gamma\"]\n",
    "atlases = ['aparc', 'aparc.a2009s']\n",
    "\n",
    "for (subject, freq, atlas) in tqdm(product(subjects, freqs, atlases)):\n",
    "    \n",
    "    if subject in subjects_control:\n",
    "        fname = f'/Users/payamsadeghishabestari/meg_gsp/stc_labels_morphed/tinmeg1/subject_{subject}_{freq}_{atlas}_mean.npy'\n",
    "    if subject in subjects_case:\n",
    "        fname = f'/Users/payamsadeghishabestari/meg_gsp/stc_labels_morphed/tinmeg3/subject_{subject}_{freq}_{atlas}_mean.npy'\n",
    "    \n",
    "    if atlas == 'aparc':\n",
    "        stc_in_labels = np.load(file=fname, allow_pickle=True)[:-1,:] # mind this\n",
    "\n",
    "    if atlas == 'aparc.a2009s':\n",
    "        stc_in_labels = np.load(file=fname, allow_pickle=True)\n",
    "    \n",
    "    graph_matrices = {}\n",
    "    graph_matrix = learn_graph.log_degree_barrier(X=stc_in_labels, dist_type='sqeuclidean', alpha=1,\n",
    "                                                beta=1, step=0.5, w0=None, maxit=10000, rtol=1e-16,\n",
    "                                                retall=False, verbosity='LOW')\n",
    "    ### extracting adjacency matrix\n",
    "    if subject in subjects_control:\n",
    "        subject = f\"0{subject}\"\n",
    "        save_dir = f\"/Users/payamsadeghishabestari/meg_gsp/all_subjects_graph_morphed/{atlas}/{freq}/control/{subject}.png\"\n",
    "    else:\n",
    "        save_dir = f\"/Users/payamsadeghishabestari/meg_gsp/all_subjects_graph_morphed/{atlas}/{freq}/case/{subject}.png\"\n",
    "\n",
    "    if atlas == 'aparc':\n",
    "        labels = mne.read_labels_from_annot(subject=\"fsaverage\", parc=atlas,\n",
    "                                            subjects_dir=subjects_dir, verbose=False)[:-1]\n",
    "    if atlas == 'aparc.a2009s':\n",
    "        labels = mne.read_labels_from_annot(subject=\"fsaverage\", parc=atlas,\n",
    "                                            subjects_dir=subjects_dir, verbose=False)[:-2]\n",
    "    \n",
    "    \n",
    "    node_coords = []\n",
    "    for label in labels:\n",
    "        if label.hemi == 'lh':\n",
    "            hemi = 0\n",
    "        if label.hemi == 'rh':\n",
    "            hemi = 1\n",
    "        center_vertex = label.center_of_mass(subject=\"fsaverage\",\n",
    "                                            restrict_vertices=False, \n",
    "                                            subjects_dir=subjects_dir)\n",
    "        mni_pos = mne.vertex_to_mni(center_vertex, hemis=hemi,\n",
    "                                subject=\"fsaverage\", subjects_dir=subjects_dir)\n",
    "        node_coords.append(mni_pos)\n",
    "\n",
    "    node_coords = np.array(node_coords)\n",
    "    ticks = [lb.name for lb in labels]\n",
    "\n",
    "    ### visualizing\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(11, 5))\n",
    "    plot_connectome(adjacency_matrix=graph_matrix, node_coords=node_coords,\n",
    "                    node_color='k', node_size=20, axes=axes,\n",
    "                    edge_threshold='90%')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_dir, format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Identifiability Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_identifiability_matrix(directory, atlas, method, freq_range, end_sample, graph_based=True,\n",
    "                                    con_method = \"pli\", graph_method=\"log\", case_or_control=\"both\",\n",
    "                                    dist_type=\"sqeuclidean\", alpha=1, beta=1, step=0.5, max_iter=10000,\n",
    "                                    rtol=1e-16, subtract_off_diag_avg=True, verbose=None):\n",
    "    \n",
    "    ## reorder labels\n",
    "    if atlas == 'aparc':\n",
    "        n_labels = 68\n",
    "    if atlas == 'aparc.a2009s':\n",
    "        n_labels = 148 \n",
    "\n",
    "    ## loading all subjects\n",
    "    fnames = []\n",
    "    if case_or_control == 'control':\n",
    "        for filename in sorted(os.listdir(directory)): \n",
    "            f = os.path.join(directory, filename)\n",
    "            if os.path.isfile(f) and f.endswith(f'{atlas}_{method}.npy') and freq_range in f:\n",
    "                fnames.append(f)\n",
    "\n",
    "    if case_or_control == 'case':\n",
    "        for filename in sorted(os.listdir(directory)): \n",
    "            f = os.path.join(directory, filename)\n",
    "            if os.path.isfile(f) and f.endswith(f'{atlas}_{method}.npy') and freq_range in f:\n",
    "                fnames.append(f)\n",
    "\n",
    "    if case_or_control == 'both':\n",
    "        for dir in directory:\n",
    "            for filename in sorted(os.listdir(dir)): \n",
    "                f = os.path.join(dir, filename)\n",
    "                if os.path.isfile(f) and f.endswith(f'{atlas}_{method}.npy') and freq_range in f:\n",
    "                    fnames.append(f)\n",
    "\n",
    "    ## reading the files and reordering labels\n",
    "    n_subjects = len(fnames)\n",
    "    half_size = end_sample // 2\n",
    "    all_tcs_s1 = []\n",
    "    all_tcs_s2 = []\n",
    "    for fname in fnames:\n",
    "        tcs = np.load(file=fname, allow_pickle=True)\n",
    "        all_tcs_s1.append(tcs[:, :half_size])  # session 1\n",
    "        all_tcs_s2.append(tcs[:, half_size:end_sample]) # session 2\n",
    "\n",
    "    vectors_s1 = []\n",
    "    vectors_s2 = []\n",
    "    up_tri_idxs = np.triu_indices(n_labels, k=1)\n",
    "\n",
    "    for tc1, tc2 in tzip(all_tcs_s1, all_tcs_s2):\n",
    "        \n",
    "        if graph_based: # learning the graph matrix for both sessions\n",
    "            if graph_method == \"log\":\n",
    "                graph_s1 = learn_graph.log_degree_barrier(X=tc1, dist_type=dist_type,\n",
    "                                                        alpha=alpha, beta=beta, step=step,\n",
    "                                                        w0=None, maxit=max_iter, rtol=rtol,\n",
    "                                                        retall=False, verbosity=verbose)\n",
    "                graph_s2 = learn_graph.log_degree_barrier(X=tc2, dist_type=dist_type,\n",
    "                                                        alpha=alpha, beta=beta, step=step,\n",
    "                                                        w0=None, maxit=max_iter, rtol=rtol,\n",
    "                                                        retall=False, verbosity=verbose)\n",
    "                \n",
    "            if graph_method == \"glasso\":\n",
    "                graph_s1 = learn_graph.glasso(X=tc1, alpha=alpha, w0=None, maxit=max_iter,\n",
    "                                            rtol=rtol, retall=False, verbosity=verbose)\n",
    "                graph_s2 = learn_graph.glasso(X=tc2, alpha=alpha, w0=None, maxit=max_iter,\n",
    "                                            rtol=rtol, retall=False, verbosity=verbose)\n",
    "                \n",
    "            if graph_method == \"l2-reg\":\n",
    "                graph_s1 = learn_graph.l2_degree_reg(X=tc1, dist_type=dist_type, alpha=alpha,\n",
    "                                                    s=None, step=step, w0=None, maxit=max_iter,\n",
    "                                                    rtol=rtol, retall=False, verbosity=verbose)\n",
    "                graph_s2 = learn_graph.l2_degree_reg(X=tc2, dist_type=dist_type, alpha=alpha,\n",
    "                                                    s=None, step=step, w0=None, maxit=max_iter,\n",
    "                                                    rtol=rtol, retall=False, verbosity=verbose)\n",
    "                \n",
    "        if not graph_based:   \n",
    "            if freq_range == \"alpha\":\n",
    "                (l_freq, h_freq) = (8, 13)\n",
    "            if freq_range == \"gamma\":\n",
    "                (l_freq, h_freq) = (30, 45)\n",
    "        \n",
    "            freqs = np.linspace(l_freq, h_freq, 5)\n",
    "            con_s1 = spectral_connectivity_time(data=tc1[np.newaxis, :], method=con_method, freqs=freqs,\n",
    "                                                average=True, mode=\"multitaper\", sfreq=sfreq, fmin=l_freq,\n",
    "                                                fmax=h_freq, faverage=True, verbose=False)\n",
    "            con_s2 = spectral_connectivity_time(data=tc2[np.newaxis, :], method=con_method, freqs=freqs,\n",
    "                                                average=True, mode=\"multitaper\", sfreq=sfreq, fmin=l_freq, \n",
    "                                                fmax=h_freq, faverage=True, verbose=False)\n",
    "            \n",
    "            graph_s1 = con_s1.get_data(output=\"dense\")[:, :, 0] + con_s1.get_data(output=\"dense\")[:, :, 0].T\n",
    "            graph_s2 = con_s2.get_data(output=\"dense\")[:, :, 0] + con_s2.get_data(output=\"dense\")[:, :, 0].T\n",
    "        \n",
    "        ## vectorizing the upper triangle and computing the correlation coeff\n",
    "        vectors_s1.append(graph_s1[up_tri_idxs])\n",
    "        vectors_s2.append(graph_s2[up_tri_idxs])\n",
    "        \n",
    "    ## filling identifiability matrix\n",
    "    id_matrix = np.zeros(shape=(n_subjects, n_subjects))\n",
    "    for i, j in itertools.product(range(n_subjects), range(n_subjects)):\n",
    "        id_matrix[i][j] = np.corrcoef(vectors_s1[i], vectors_s2[j])[0, 1]\n",
    "\n",
    "    if subtract_off_diag_avg:\n",
    "        for i, j in itertools.product(range(n_subjects), range(n_subjects)):\n",
    "            if i == j:\n",
    "                id_matrix[i][j] = id_matrix[i][j] - (sum(id_matrix[i]) - id_matrix[i][j]) / (n_subjects - 1)\n",
    "\n",
    "    return id_matrix\n",
    "\n",
    "def compute_rank(id_matrix):\n",
    "    ranks = []\n",
    "    for row_idx, row in enumerate(id_matrix):\n",
    "        ranks.append(np.where(sorted(row, reverse=True) == row[row_idx])[0][0] + 1)   \n",
    "    ranks_control = ranks[:23]\n",
    "    ranks_case = ranks[23:]\n",
    "    return ranks_control, ranks_case "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Run for one condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir1 = '/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg1'\n",
    "dir2 = '/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg3'\n",
    "method = 'mean' # or mean_flip\n",
    "case_or_control = 'both' # or control or both\n",
    "sfreq = 250\n",
    "end_sample = 5 * 60 * sfreq \n",
    "dist_type = \"sqeuclidean\"\n",
    "alpha = 1\n",
    "step = 0.5\n",
    "max_iter = 10000\n",
    "rtol = 1e-16\n",
    "beta = 1\n",
    "atlas = 'aparc'\n",
    "freq_range = 'alpha'\n",
    "id_matrix = compute_identifiability_matrix(directory=[dir1, dir2], atlas=atlas, method=method, graph_based=False,\n",
    "                                        con_method = \"pli\", freq_range=freq_range, end_sample=end_sample,\n",
    "                                        graph_method=\"log\", case_or_control=\"both\", dist_type=dist_type, alpha=alpha,\n",
    "                                        beta=beta, step=step, max_iter=max_iter, rtol=rtol, \n",
    "                                        subtract_off_diag_avg=False, verbose=\"NONE\")\n",
    "plt.figure()\n",
    "plt.imshow(id_matrix)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making big Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir1 = '/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg1'\n",
    "dir2 = '/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg3'\n",
    "method = 'mean' # or mean_flip\n",
    "case_or_control = 'both' # or control or both\n",
    "sfreq = 250\n",
    "end_sample = 5 * 60 * sfreq \n",
    "dist_type = \"sqeuclidean\"\n",
    "alpha = 1\n",
    "step = 0.5\n",
    "max_iter = 10000\n",
    "rtol = 1e-16\n",
    "betas = [0.5, 1, 1.5]\n",
    "atlases = ['aparc', 'aparc.a2009s']\n",
    "freq_ranges = ['alpha', 'gamma']\n",
    "\n",
    "identifiability_dict = {}\n",
    "for freq_range in freq_ranges:\n",
    "    identifiability_dict[freq_range] = {}\n",
    "    for atlas in atlases:\n",
    "        identifiability_dict[freq_range][atlas] = {}\n",
    "        for beta in betas:\n",
    "            id_matrix = compute_identifiability_matrix(directory=[dir1, dir2], atlas=atlas, method=method,\n",
    "                                        freq_range=freq_range, end_sample=end_sample, graph_method=\"log\",\n",
    "                                        case_or_control=\"both\", dist_type=dist_type, alpha=alpha,\n",
    "                                        subtract_off_diag_avg=False, beta=beta, step=step, max_iter=max_iter,\n",
    "                                        rtol=rtol, verbose=\"NONE\")\n",
    "            identifiability_dict[freq_range][atlas][beta] = id_matrix\n",
    "\n",
    "with open('/Users/payamsadeghishabestari/meg_gsp/identifiability_no_subtract.pkl', 'wb') as file:\n",
    "    pickle.dump(identifiability_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading\n",
    "file_path = '/Users/payamsadeghishabestari/meg_gsp/identifiability_no_subtract.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    identifiability_dict = pickle.load(file)\n",
    "\n",
    "line_x = np.linspace(-0.5, 41.5, 100)\n",
    "\n",
    "## visualizing \n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "ax.plot(line_x, [17.5]*len(line_x), 'grey', linewidth=3)\n",
    "ax.plot([17.5]*len(line_x), line_x, 'grey', linewidth=3)\n",
    "\n",
    "ax.plot(line_x, [40.5]*len(line_x), 'grey', linewidth=3)\n",
    "ax.plot([40.5]*len(line_x), line_x, 'grey', linewidth=3)\n",
    "ax.plot(line_x, [-0.5]*len(line_x), 'grey', linewidth=3)\n",
    "ax.plot([-0.5]*len(line_x), line_x, 'grey', linewidth=3)\n",
    "\n",
    "im = ax.imshow(identifiability_dict['gamma']['aparc'][1], cmap=\"coolwarm\", vmin=0, vmax=1)\n",
    "divider = make_axes_locatable(ax)\n",
    "# cax = divider.append_axes('right', size='5%', pad=0.1)\n",
    "# fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.axis('off')\n",
    "# fig.savefig(fname=\"/Users/payamsadeghishabestari/meg_gsp/papar_plots/id_matrix_alpha.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, figsize=(10, 6))\n",
    "\n",
    "id_matrix = identifiability_dict['gamma']['aparc'][0.5]\n",
    "ranks_control, ranks_case = compute_rank(id_matrix)\n",
    "ranks_co = [ranks_control.count(i) for i in range(1, 42)]\n",
    "ranks_ca = [ranks_case.count(i) for i in range(1, 42)]\n",
    "bar1 = axs[0][0].bar(range(1, 42), ranks_co, label='Control')\n",
    "bar2 = axs[0][0].bar(range(1, 42), ranks_ca, label='Case', bottom=ranks_co)\n",
    "\n",
    "id_matrix = identifiability_dict['gamma']['aparc'][1]\n",
    "ranks_control, ranks_case = compute_rank(id_matrix)\n",
    "ranks_co = [ranks_control.count(i) for i in range(1, 42)]\n",
    "ranks_ca = [ranks_case.count(i) for i in range(1, 42)]\n",
    "bar1 = axs[1][0].bar(range(1, 42), ranks_co, label='Control')\n",
    "bar2 = axs[1][0].bar(range(1, 42), ranks_ca, label='Case', bottom=ranks_co)\n",
    "\n",
    "id_matrix = identifiability_dict['gamma']['aparc'][1.5]\n",
    "ranks_control, ranks_case = compute_rank(id_matrix)\n",
    "ranks_co = [ranks_control.count(i) for i in range(1, 42)]\n",
    "ranks_ca = [ranks_case.count(i) for i in range(1, 42)]\n",
    "bar1 = axs[2][0].bar(range(1, 42), ranks_co, label='Control')\n",
    "bar2 = axs[2][0].bar(range(1, 42), ranks_ca, label='Case', bottom=ranks_co)\n",
    "\n",
    "id_matrix = identifiability_dict['alpha']['aparc.a2009s'][0.5]\n",
    "ranks_control, ranks_case = compute_rank(id_matrix)\n",
    "ranks_co = [ranks_control.count(i) for i in range(1, 42)]\n",
    "ranks_ca = [ranks_case.count(i) for i in range(1, 42)]\n",
    "bar1 = axs[0][1].bar(range(1, 42), ranks_co, label='Control')\n",
    "bar2 = axs[0][1].bar(range(1, 42), ranks_ca, label='Case', bottom=ranks_co)\n",
    "\n",
    "id_matrix = identifiability_dict['alpha']['aparc.a2009s'][1]\n",
    "ranks_control, ranks_case = compute_rank(id_matrix)\n",
    "ranks_co = [ranks_control.count(i) for i in range(1, 42)]\n",
    "ranks_ca = [ranks_case.count(i) for i in range(1, 42)]\n",
    "bar1 = axs[1][1].bar(range(1, 42), ranks_co, label='Control')\n",
    "bar2 = axs[1][1].bar(range(1, 42), ranks_ca, label='Case', bottom=ranks_co)\n",
    "\n",
    "id_matrix = identifiability_dict['alpha']['aparc.a2009s'][1.5]\n",
    "ranks_control, ranks_case = compute_rank(id_matrix)\n",
    "ranks_co = [ranks_control.count(i) for i in range(1, 42)]\n",
    "ranks_ca = [ranks_case.count(i) for i in range(1, 42)]\n",
    "bar1 = axs[2][1].bar(range(1, 42), ranks_co, label='Control')\n",
    "bar2 = axs[2][1].bar(range(1, 42), ranks_ca, label='Case', bottom=ranks_co)\n",
    "\n",
    "for i, j in itertools.product(range(3), range(2)):\n",
    "    axs[i][j].set_xticks(range(1, 10))\n",
    "    axs[i][j].set_xlim([0, 10])\n",
    "    axs[i][j].set_ylim([0, 41])\n",
    "    axs[i][j].legend(loc='upper right', frameon=False)\n",
    "\n",
    "axs[0][0].set_title('aparc (68 lbs)')\n",
    "axs[0][1].set_title('aparc (148 lbs)')\n",
    "axs[0][0].set_ylabel('beta = 0.5')\n",
    "axs[1][0].set_ylabel('beta = 1.0')\n",
    "axs[2][0].set_ylabel('beta = 1.5')\n",
    "axs[2][0].set_xlabel('ranks')\n",
    "axs[2][1].set_xlabel('ranks')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette('Set1')[:2]\n",
    "X = ['Rank 1','Rank 2','Rank 3','Rank 4', \"Rank 5\"] \n",
    "X_axis = np.arange(len(X)) \n",
    "\n",
    "## alpha (0.5)\n",
    "controls_rank = [23,0,0,0,0] \n",
    "cases_rank = [16,0,0,2,0] \n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "ax.bar(X_axis - 0.2, cases_rank, 0.4, label = 'Girls', color=colors[0]) \n",
    "ax.bar(X_axis + 0.2, controls_rank, 0.4, label = 'Boys', color=colors[1]) \n",
    "ax.set_xticks(X_axis, X) \n",
    "ax.set_title(\"freq_alpha, beta=0.5\") \n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_ylim([0, 25])\n",
    "\n",
    "## alpha (1)\n",
    "controls_rank = [22,1,0,0,0] \n",
    "cases_rank = [16,1,1,0,0] \n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "ax.bar(X_axis - 0.2, cases_rank, 0.4, label = 'Girls', color=colors[0]) \n",
    "ax.bar(X_axis + 0.2, controls_rank, 0.4, label = 'Boys', color=colors[1]) \n",
    "ax.set_xticks(X_axis, X) \n",
    "ax.set_title(\"freq_alpha, beta=1\") \n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_ylim([0, 25])\n",
    "\n",
    "## alpha (1.5)\n",
    "controls_rank = [22,1,0,0,0] \n",
    "cases_rank = [16,1,1,0,0] \n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "ax.bar(X_axis - 0.2, cases_rank, 0.4, label = 'Case', color=colors[0]) \n",
    "ax.bar(X_axis + 0.2, controls_rank, 0.4, label = 'Control', color=colors[1]) \n",
    "ax.set_xticks(X_axis, X) \n",
    "ax.set_title(\"freq_alpha, beta=1.5\") \n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.legend(frameon=False)\n",
    "ax.set_ylim([0, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg1/subject_756_alpha_aparc_mean.npy\"\n",
    "tcs = np.load(file=fname, allow_pickle=True)\n",
    "half_size = int(len(tcs[0]) / 2)\n",
    "tc1 = tcs[:, :half_size]  # session 1\n",
    "tc2 = tcs[:, half_size:] # session 2\n",
    "\n",
    "dist_type = \"sqeuclidean\"\n",
    "alpha = 1\n",
    "beta = 1\n",
    "step = 0.5\n",
    "max_iter = 10000\n",
    "rtol = 1e-16\n",
    "\n",
    "graph_s1 = learn_graph.log_degree_barrier(X=tc1, dist_type=dist_type,\n",
    "                                        alpha=alpha, beta=beta, step=step,\n",
    "                                        w0=None, maxit=max_iter, rtol=rtol,\n",
    "                                        retall=False)\n",
    "graph_s2 = learn_graph.log_degree_barrier(X=tc2, dist_type=dist_type,\n",
    "                                        alpha=alpha, beta=beta, step=step,\n",
    "                                        w0=None, maxit=max_iter, rtol=rtol,\n",
    "                                        retall=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = graph_s1 / graph_s1.max()\n",
    "mask1 = np.tri(A.shape[0], k=-1)\n",
    "A = np.ma.array(A, mask=1-mask1) # mask out the lower triangle\n",
    "\n",
    "B = graph_s2 / graph_s2.max()\n",
    "mask2 = np.tri(B.shape[0], k=0)\n",
    "B = np.ma.array(B, mask=mask2) # mask out the lower triangle\n",
    "\n",
    "fig, ax1 = plt.subplots(1,1)\n",
    "\n",
    "im1 = ax1.imshow(A, cmap=\"afmhot\", vmin=0, vmax=0.7)\n",
    "im1 = ax1.imshow(B, cmap=\"afmhot\", vmin=0, vmax=0.7)\n",
    "\n",
    "ax1.spines['left'].set_visible(False)\n",
    "ax1.spines['bottom'].set_visible(False)\n",
    "ax1.axis('off')\n",
    "fig.colorbar(mappable=im1, ax=ax1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising brains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra plot\n",
    "Brain = mne.viz.get_brain_class()\n",
    "clr = 0.85\n",
    "brain_kwargs = dict(alpha=1, background=\"white\", cortex=[(clr, clr, clr), (clr, clr, clr)], size=(800, 600), views='lateral')\n",
    "brain_labels = mne.read_labels_from_annot(subject='fsaverage', parc='aparc')[:-1] # 69 labels\n",
    "brain = Brain(\"fsaverage\", hemi=\"lh\", surf=\"pial_semi_inflated\", **brain_kwargs)\n",
    "brain.add_annotation(annot=\"aparc\", borders=False, alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading subjects\n",
    "atlas = 'aparc'\n",
    "freq_range = 'alpha'\n",
    "method = 'mean'\n",
    "dirs = ['/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg1',\n",
    "        '/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg3']\n",
    "fnames = []\n",
    "for dir in dirs:\n",
    "    for filename in sorted(os.listdir(dir)): \n",
    "        f = os.path.join(dir, filename)\n",
    "        if os.path.isfile(f) and f.endswith(f'{atlas}_{method}.npy') and freq_range in f:\n",
    "            fnames.append(f)\n",
    "\n",
    "# creating vectors for each time period\n",
    "print(f'Computing the Graph Matrixes ...')\n",
    "sfreq = 250\n",
    "init_time = 2\n",
    "end_time = 300\n",
    "n_frames = 100\n",
    "if atlas == 'aparc':\n",
    "    n_labels = 68\n",
    "if atlas == 'aparc.a2009s':\n",
    "    n_labels = 148\n",
    "up_tri_idxs = np.triu_indices(n_labels, k=1)\n",
    "durations = (np.linspace(init_time, end_time, n_frames) * sfreq).astype(int)\n",
    "all_vectors = []\n",
    "for fname in tqdm(fnames):\n",
    "    \n",
    "    # vectors = np.zeros(shape=(len(durations), len(up_tri_idxs[0])))\n",
    "    vectors = np.zeros(shape=(len(durations), len(label_idxs))) # run this only if you want to use significant edges\n",
    "    \n",
    "    stc_in_labels = np.load(file=fname, allow_pickle=True)\n",
    "    for idx, duration in enumerate(durations):\n",
    "        graph = learn_graph.log_degree_barrier(X=stc_in_labels[:, :duration],\n",
    "                                                dist_type='sqeuclidean', alpha=1,\n",
    "                                                beta=1, step=0.5, w0=None, maxit=10000,\n",
    "                                                rtol=1e-16, retall=False, verbosity='NONE')\n",
    "        graph = graph / np.linalg.norm(graph, 'fro') # normalizing it\n",
    "        \n",
    "        # vectors[idx] = graph[up_tri_idxs]\n",
    "        vectors[idx] = graph[tuple(np.array(label_idxs).T)] # run this only if you want to use significant edges\n",
    "    \n",
    "    all_vectors.append(vectors)\n",
    "\n",
    "all_vectors = np.array(all_vectors)\n",
    "\n",
    "# going to umap space\n",
    "print(f'Creating the UMAP space data ...')\n",
    "data_transformed = []\n",
    "for i in tqdm(range(n_frames)):\n",
    "    data = torch.tensor(all_vectors[:,i,:], dtype=torch.float32)\n",
    "    reshaped_data = data.reshape(data.shape[0], -1)\n",
    "    umap_transform = UMAP(n_components=2, n_neighbors=3, \n",
    "                    learning_rate = 1e-7, metric='euclidean')\n",
    "    data_transformed.append(umap_transform.fit_transform(reshaped_data))\n",
    "\n",
    "# creating the animation\n",
    "colors = ['b'] * 23 + ['r'] * 18 # control vs case\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(-2, 12)\n",
    "ax.set_ylim(-2, 12)\n",
    "ax.grid()\n",
    "ax.set_xlabel('UMAP componet 1')\n",
    "ax.set_ylabel('UMAP componet 2')\n",
    "scatter = ax.scatter([], [], s=25)\n",
    "text = ax.text(-1.5, -1.5, '', fontsize='large', fontstyle='italic')\n",
    "def update(frame):\n",
    "    x = data_transformed[frame][:, 0]\n",
    "    y = data_transformed[frame][:, 1]\n",
    "    offsets = np.column_stack((x, y))\n",
    "    scatter.set_offsets(offsets)\n",
    "    scatter.set_color(colors)\n",
    "    text.set_text(f'Frame {frame}/{n_frames}')\n",
    "    return scatter,\n",
    "\n",
    "animation = FuncAnimation(fig, update, frames=n_frames, init_func=None, repeat=False, interval=300)\n",
    "plt.show()\n",
    "\n",
    "# saving the animation\n",
    "animation.save(f\"/Users/payamsadeghishabestari/meg_gsp/{freq_range}_{atlas}_{method}_significant_umap.gif\", writer=\"pillow\")\n",
    "plt.close(animation._fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing 2 component PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading subjects\n",
    "atlas = 'aparc'\n",
    "freq_range = 'alpha'\n",
    "method = 'mean'\n",
    "dirs = ['/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg1',\n",
    "        '/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg3']\n",
    "fnames = []\n",
    "for dir in dirs:\n",
    "    for filename in sorted(os.listdir(dir)): \n",
    "        f = os.path.join(dir, filename)\n",
    "        if os.path.isfile(f) and f.endswith(f'{atlas}_{method}.npy') and freq_range in f:\n",
    "            fnames.append(f)\n",
    "\n",
    "# creating vectors for each time period\n",
    "print(f'Computing the Graph Matrixes ...')\n",
    "sfreq = 250\n",
    "init_time = 2\n",
    "end_time = 300\n",
    "n_frames = 6\n",
    "if atlas == 'aparc':\n",
    "    n_labels = 68\n",
    "if atlas == 'aparc.a2009s':\n",
    "    n_labels = 148\n",
    "up_tri_idxs = np.triu_indices(n_labels, k=1)\n",
    "durations = (np.linspace(init_time, end_time, n_frames) * sfreq).astype(int)\n",
    "all_vectors = []\n",
    "for fname in tqdm(fnames):\n",
    "    vectors = np.zeros(shape=(len(durations), len(up_tri_idxs[0])))\n",
    "    # vectors = np.zeros(shape=(len(durations), len(label_idxs))) # run this only if you want to use significant edges\n",
    "    \n",
    "    stc_in_labels = np.load(file=fname, allow_pickle=True)\n",
    "    for idx, duration in enumerate(durations):\n",
    "        graph = learn_graph.log_degree_barrier(X=stc_in_labels[:, :duration],\n",
    "                                                dist_type='sqeuclidean', alpha=1,\n",
    "                                                beta=1, step=0.5, w0=None, maxit=10000,\n",
    "                                                rtol=1e-16, retall=False, verbosity='NONE')\n",
    "        graph = graph / np.linalg.norm(graph, 'fro') # normalizing it\n",
    "        \n",
    "        vectors[idx] = graph[up_tri_idxs]\n",
    "        # vectors[idx] = graph[tuple(np.array(label_idxs).T)] # run this only if you want to use significant edges\n",
    "\n",
    "    all_vectors.append(vectors)\n",
    "\n",
    "all_vectors = np.array(all_vectors)\n",
    "\n",
    "# computing 2 - PCA \n",
    "print(f'Computing the PCA ...')\n",
    "data_transformed = []\n",
    "for i in tqdm(range(n_frames)):\n",
    "    pca = PCA(n_components=2)\n",
    "    data_transformed.append(pca.fit_transform(all_vectors[:,i,:]))\n",
    "\n",
    "# creating the animation\n",
    "colors = ['b'] * 23 + ['r'] * 18 # control vs case\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_ylim(-5, 5)\n",
    "ax.grid()\n",
    "ax.set_xlabel('PCA componet 1')\n",
    "ax.set_ylabel('PCA componet 2')\n",
    "scatter = ax.scatter([], [], s=25)\n",
    "text = ax.text(-4.5, -4.5, '', fontsize='large', fontstyle='italic')\n",
    "def update(frame):\n",
    "    x = data_transformed[frame][:, 0]\n",
    "    y = data_transformed[frame][:, 1]\n",
    "    offsets = np.column_stack((x, y))\n",
    "    scatter.set_offsets(offsets)\n",
    "    scatter.set_color(colors)\n",
    "    text.set_text(f'Frame {frame}/{n_frames}')\n",
    "    return scatter,\n",
    "\n",
    "animation = FuncAnimation(fig, update, frames=n_frames, init_func=None, repeat=False, interval=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the animation\n",
    "colors = ['b'] * 23 + ['r'] * 18 # control vs case\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(-.5, .5)\n",
    "ax.set_ylim(-.5, .5)\n",
    "ax.grid()\n",
    "ax.set_xlabel('PCA componet 1')\n",
    "ax.set_ylabel('PCA componet 2')\n",
    "scatter = ax.scatter([], [], s=25)\n",
    "text = ax.text(-0.5, -0.5, '', fontsize='large', fontstyle='italic')\n",
    "def update(frame):\n",
    "    x = data_transformed[frame][:, 0]\n",
    "    y = data_transformed[frame][:, 1]\n",
    "    offsets = np.column_stack((x, y))\n",
    "    scatter.set_offsets(offsets)\n",
    "    scatter.set_color(colors)\n",
    "    text.set_text(f'Frame {frame}/{n_frames}')\n",
    "    return scatter,\n",
    "\n",
    "animation = FuncAnimation(fig, update, frames=n_frames, init_func=None, repeat=False, interval=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare networks statistically between two groups (t-test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters to adjust\n",
    "alpha_param = 1\n",
    "beta_param = 1\n",
    "dist_type = \"sqeuclidean\"\n",
    "\n",
    "statistics = \"t-test\" # \"Mann-Whitney U test\"\n",
    "F = 300\n",
    "sfreq = 250\n",
    "alpha = 0.05  # significance level\n",
    "p_thr = 0.05\n",
    "end_sample = F * sfreq\n",
    "\n",
    "# loading subjects\n",
    "atlas = 'aparc'\n",
    "freq_ranges = [\"delta\", \"theta\", \"alpha\", \"beta\", \"gamma\"]\n",
    "method = 'mean'\n",
    "dirs = ['/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg1',\n",
    "        '/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg3']\n",
    "\n",
    "labels = mne.read_labels_from_annot(subject='fsaverage', parc=atlas, subjects_dir=None, verbose=False)[:-1]\n",
    "lb_names = np.array([lb.name for lb in labels])\n",
    "\n",
    "stats_dict = {}\n",
    "pvals_dict = {}\n",
    "for freq_range in freq_ranges[4:]:    \n",
    "    fnames = []\n",
    "    for dir in dirs:\n",
    "        for filename in sorted(os.listdir(dir)): \n",
    "            f = os.path.join(dir, filename)\n",
    "            if os.path.isfile(f) and f.endswith(f'{atlas}_{method}.npy') and freq_range in f:\n",
    "                fnames.append(f)\n",
    "\n",
    "    # creating graphs and vectors\n",
    "    print(f'Computing the Graph Matrixes ...')\n",
    "    if atlas == 'aparc':\n",
    "        n_labels = 68\n",
    "    if atlas == 'aparc.a2009s':\n",
    "        n_labels = 148\n",
    "    up_tri_idxs = np.triu_indices(n_labels, k=1)\n",
    "    vectors = []\n",
    "    for fname in tqdm(fnames):\n",
    "        stc_in_labels = np.load(file=fname, allow_pickle=True)\n",
    "        graph = learn_graph.log_degree_barrier(X=stc_in_labels[:,:end_sample], dist_type=dist_type,\n",
    "                                            alpha=alpha_param, beta=beta_param, step=0.5, w0=None, maxit=10000,\n",
    "                                            rtol=1e-16, retall=False, verbosity='NONE')\n",
    "        graph = graph / np.linalg.norm(graph, 'fro') # normalizing it\n",
    "        vector = graph[up_tri_idxs]\n",
    "        vectors.append(vector)\n",
    "    vectors = np.array(vectors)\n",
    "\n",
    "    # removing very small connections\n",
    "    for i in range(len(fnames)):\n",
    "        zero_edges = np.where(vectors[i] < vectors[i].max() * 0.01)[0]\n",
    "        vectors[i][zero_edges] = 0\n",
    "\n",
    "    # statistics\n",
    "    control_group = vectors[:23,:]\n",
    "    case_group = vectors[23:,:]\n",
    "    if statistics == 't-test':\n",
    "        stat, p_values = ttest_ind(control_group, case_group,\n",
    "                                    permutations=0, random_state=42)\n",
    "    if statistics == 'Mann-Whitney U test':\n",
    "        stat, p_values = mannwhitneyu(control_group, case_group)\n",
    "\n",
    "    my_stat_dict = {}\n",
    "    p_dict = {}\n",
    "    methods = ['bonferroni', 'fdr_bh']\n",
    "    for method in methods:\n",
    "        reject_null, p_corrected, _, _ = sm.stats.multipletests(pvals=p_values, alpha=alpha,\n",
    "                                                            method=method)\n",
    "        label_idxs = []\n",
    "        if len(np.where(p_corrected < p_thr)[0]) == 0:\n",
    "            print(f\"No statistical difference between brain labels for method {method}\")\n",
    "        else:\n",
    "            for idx in np.where(p_corrected < p_thr)[0]:\n",
    "                label_idxs.append((up_tri_idxs[0][idx], up_tri_idxs[1][idx]))\n",
    "        my_stat_dict[method] = lb_names[label_idxs]\n",
    "        p_dict[method] = p_corrected\n",
    "    \n",
    "    stats_dict[freq_range] = my_stat_dict\n",
    "    pvals_dict[freq_range] = p_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats_dict[\"gamma\"][\"fdr_bh\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the distribution of significant edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {\"group\": [], \"connection\": []}\n",
    "up_tri_idxs = np.triu_indices(n_labels, k=1)\n",
    "(lb1, lb2) = (\"insula-lh\", \"parahippocampal-lh\")\n",
    "\n",
    "lb1_idx = list(lb_names).index(lb1)\n",
    "lb2_idx = list(lb_names).index(lb2)\n",
    "idx = np.where((up_tri_idxs[0] == min(lb1_idx, lb2_idx)) & (up_tri_idxs[1] == max(lb1_idx, lb2_idx)))[0]\n",
    "\n",
    "for sub in control_group:\n",
    "    my_dict[\"connection\"].append(sub[idx][0])\n",
    "    my_dict[\"group\"].append(\"control\")\n",
    "for sub in case_group:\n",
    "    my_dict[\"connection\"].append(sub[idx][0])\n",
    "    my_dict[\"group\"].append(\"case\")\n",
    "\n",
    "df = pd.DataFrame(my_dict)\n",
    "\n",
    "palette_color = ['#1f77b4', '#d62728']\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "sns.violinplot(data=df, x=\"group\", y=\"connection\", saturation=0.75, palette=palette_color, fill=False, ax=ax, inner=None)\n",
    "sns.swarmplot(data=df, x=\"group\", y=\"connection\", palette=palette_color, ax=ax)\n",
    "ax.set_title(f\"connection between {lb1} vs {lb2}\")\n",
    "ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare networks statistically between two groups (k-sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters to adjust\n",
    "alpha_param = 1\n",
    "beta_param = 1\n",
    "dist_type = \"sqeuclidean\"\n",
    "\n",
    "statistics = \"t-test\" # \"Mann-Whitney U test\"\n",
    "F = 300\n",
    "sfreq = 250\n",
    "alpha = 0.05  # significance level\n",
    "p_thr = 0.05\n",
    "end_sample = F * sfreq\n",
    "\n",
    "# loading subjects\n",
    "atlas = 'aparc'\n",
    "freq_ranges = [\"delta\", \"theta\", \"alpha\", \"beta\", \"gamma\"]\n",
    "method = 'mean'\n",
    "dirs = ['/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg1',\n",
    "        '/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg3']\n",
    "\n",
    "labels = mne.read_labels_from_annot(subject='fsaverage', parc=atlas, subjects_dir=None, verbose=False)[:-1]\n",
    "lb_names = np.array([lb.name for lb in labels])\n",
    "\n",
    "stats_dict = {}\n",
    "pvals_dict = {}\n",
    "for freq_range in freq_ranges[2:3]:    \n",
    "    fnames = []\n",
    "    for dir in dirs:\n",
    "        for filename in sorted(os.listdir(dir)): \n",
    "            f = os.path.join(dir, filename)\n",
    "            if os.path.isfile(f) and f.endswith(f'{atlas}_{method}.npy') and freq_range in f:\n",
    "                fnames.append(f)\n",
    "\n",
    "    # creating graphs and vectors\n",
    "    print(f'Computing the Graph Matrixes ...')\n",
    "    if atlas == 'aparc':\n",
    "        n_labels = 68\n",
    "    if atlas == 'aparc.a2009s':\n",
    "        n_labels = 148\n",
    "    up_tri_idxs = np.triu_indices(n_labels, k=1)\n",
    "    graphs = []\n",
    "    for fname in tqdm(fnames):\n",
    "        stc_in_labels = np.load(file=fname, allow_pickle=True)\n",
    "        graph = learn_graph.log_degree_barrier(X=stc_in_labels[:,:end_sample], dist_type=dist_type,\n",
    "                                            alpha=alpha_param, beta=beta_param, step=0.5, w0=None, maxit=10000,\n",
    "                                            rtol=1e-16, retall=False, verbosity='NONE')\n",
    "        graph = graph / np.linalg.norm(graph, 'fro') # normalizing it\n",
    "        \n",
    "        thr = 0.01 * np.max(graph)\n",
    "        graph[graph < thr] = 0\n",
    "\n",
    "        graphs.append(graph)\n",
    "    graphs = np.array(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"bonferroni\"\n",
    "samples_co = graphs[:23]\n",
    "samples_ca = graphs[23:]\n",
    "connectomes = [samples_co, samples_ca]\n",
    "\n",
    "indices = zip(*np.triu_indices(68, 1))\n",
    "edge_pvals = []\n",
    "for roi_i, roi_j in indices:\n",
    "\n",
    "    # Get the (i,j)-th edge for each connectome\n",
    "    samples = [type[:, roi_i, roi_j] for type in connectomes]\n",
    "\n",
    "    # Calculate the p-value for the (i,j)-th edge\n",
    "    try:\n",
    "        statistic, pvalue = KSample(\"Dcorr\").test(*samples, reps=10000, workers=-1)\n",
    "    except ValueError:\n",
    "        # A ValueError is thrown when any of the samples have equal edge\n",
    "        # weights (i.e. one of the inputs has 0 variance)\n",
    "        statistic = np.nan\n",
    "        pvalue = 1\n",
    "\n",
    "    edge_pvals.append([roi_i, roi_j, statistic, pvalue])\n",
    "\n",
    "# Convert the nested list to a dataframe\n",
    "edge_pvals_df = pd.DataFrame(edge_pvals, columns=[\"ROI_1\", \"ROI_2\", \"stat\", \"pval\"])\n",
    "\n",
    "# multiple comparison\n",
    "alpha = 0.05\n",
    "ps = [item[3] for item in edge_pvals]\n",
    "reject_null, corr_ps, _, _ = sm.stats.multipletests(pvals=ps, alpha=alpha,\n",
    "                                                        method=method)\n",
    "for p_idx, p in enumerate(corr_ps):\n",
    "    edge_pvals[p_idx][3] = p\n",
    "\n",
    "edge_pvals_df[\"corr_pval\"] = corr_ps\n",
    "edge_pvals_df[\"significant\"] = (edge_pvals_df[\"corr_pval\"] < alpha)\n",
    "\n",
    "# Get the top 10 strongest signal edges\n",
    "edge_pvals_df.sort_values(by=\"corr_pval\", inplace=True, ignore_index=True)\n",
    "edge_pvals_top = edge_pvals_df.head(10)\n",
    "\n",
    "# Replace ROI indices with actual names\n",
    "labels = mne.read_labels_from_annot(subject=\"fsaverage\", parc=\"aparc\",\n",
    "                                    subjects_dir=None, verbose=False)[:-1]\n",
    "ticks = [lb.name for lb in labels]\n",
    "def lookup_roi_name(index):\n",
    "    roi_name = ticks[index]\n",
    "    return f\"{roi_name}\"\n",
    "\n",
    "edge_pvals_top[\"ROI_1\"] = edge_pvals_top[\"ROI_1\"].apply(lookup_roi_name)\n",
    "edge_pvals_top[\"ROI_2\"] = edge_pvals_top[\"ROI_2\"].apply(lookup_roi_name)\n",
    "edge_pvals_top.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GMM check\n",
    "two_groups = np.concatenate((control_group, case_group), axis=0)\n",
    "aics_1 = []; aics_2 = []\n",
    "bics_1 = []; bics_2 = []\n",
    "for edge in tqdm(two_groups.T):\n",
    "    edges = edge.reshape(-1, 1)\n",
    "    # 1 component\n",
    "    gmm = GaussianMixture(n_components=1)\n",
    "    gmm.fit(edges)\n",
    "    aics_1.append(gmm.aic(edges))\n",
    "    bics_1.append(gmm.bic(edges))\n",
    "    # 2 component\n",
    "    gmm = GaussianMixture(n_components=2)\n",
    "    gmm.fit(edges)\n",
    "    aics_2.append(gmm.aic(edges))\n",
    "    bics_2.append(gmm.bic(edges))\n",
    "\n",
    "label_idxs = []\n",
    "for idx, (aic_1, aic_2) in enumerate(zip(aics_1, aics_2)):\n",
    "    if aic_2 < (aic_1 * 1.2): # can put more constraint here now 10 %\n",
    "        label_idxs.append((up_tri_idxs[0][idx], up_tri_idxs[1][idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "control_avg_vector = np.mean(vectors[:23,:], axis=0)\n",
    "control_avg_graph = np.zeros(shape=(n_labels, n_labels))\n",
    "control_avg_graph[up_tri_idxs] = control_avg_vector\n",
    "control_avg_graph = control_avg_graph + control_avg_graph.T\n",
    "control_avg_graph = control_avg_graph / np.linalg.norm(control_avg_graph, 'fro') # normalizing it\n",
    "\n",
    "case_avg_vector = np.mean(vectors[23:,:], axis=0)\n",
    "case_avg_graph = np.zeros(shape=(n_labels, n_labels))\n",
    "case_avg_graph[up_tri_idxs] = case_avg_vector\n",
    "case_avg_graph = case_avg_graph + case_avg_graph.T\n",
    "case_avg_graph = case_avg_graph / np.linalg.norm(case_avg_graph, 'fro') # normalizing it\n",
    "\n",
    "diff_graph = control_avg_graph - case_avg_graph\n",
    "\n",
    "labels = mne.read_labels_from_annot(subject='fsaverage', parc=atlas, subjects_dir=None, verbose=False)\n",
    "if atlas == 'aparc.a2009s':\n",
    "    labels = labels[:-2]\n",
    "if atlas == 'aparc':\n",
    "    labels = labels[:-1]\n",
    "\n",
    "node_coords = []\n",
    "for label in labels:\n",
    "    if label.hemi == 'lh':\n",
    "        hemi = 0\n",
    "    if label.hemi == 'rh':\n",
    "        hemi = 1\n",
    "    center_vertex = label.center_of_mass(subject='fsaverage', \n",
    "                                        restrict_vertices=False, \n",
    "                                        subjects_dir=None)\n",
    "    mni_pos = mne.vertex_to_mni(center_vertex, hemis=hemi,\n",
    "                            subject='fsaverage', subjects_dir=None)\n",
    "    node_coords.append(mni_pos)\n",
    "\n",
    "node_coords = np.array(node_coords)\n",
    "ticks = [lb.name for lb in labels]\n",
    "\n",
    "diff_graph = control_avg_graph - case_avg_graph\n",
    "zero_matrix = np.zeros(shape=(n_labels, n_labels))\n",
    "for i, j in my_stat_dict[\"bonferroni\"]:\n",
    "    zero_matrix[i][j] = diff_graph[i][j]\n",
    "stat_graph = zero_matrix + zero_matrix.T  \n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(11, 5))\n",
    "plot_connectome(adjacency_matrix=stat_graph, node_coords=node_coords, display_mode=\"lzry\",\n",
    "                node_color='k', node_size=20, axes=ax, colorbar=False,\n",
    "                edge_threshold=\"90%\")\n",
    "fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between averages of groups and one per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = \"demean-2norm\"\n",
    "sfreq = 250\n",
    "end_sample = 5 * 60 * sfreq \n",
    "atlas = 'aparc'\n",
    "freq_range = 'alpha'\n",
    "method = 'mean'\n",
    "dirs = ['/Users/payamsadeghishabestari/meg_gsp/stc_labels_morphed/tinmeg1',\n",
    "        '/Users/payamsadeghishabestari/meg_gsp/stc_labels_morphed/tinmeg3']\n",
    "subjects_dir = '/Applications/freesurfer/7.4.1/subjects'\n",
    "fnames = []\n",
    "for dir in dirs:\n",
    "    for filename in sorted(os.listdir(dir)): \n",
    "        f = os.path.join(dir, filename)\n",
    "        if os.path.isfile(f) and f.endswith(f'{atlas}_{method}.npy') and freq_range in f:\n",
    "            fnames.append(f)\n",
    "\n",
    "print(f'Computing the Graph Matrixes ...')\n",
    "if atlas == 'aparc':\n",
    "    n_labels = 68\n",
    "if atlas == 'aparc.a2009s':\n",
    "    n_labels = 148\n",
    "\n",
    "# average of case and control subjects\n",
    "graphs = []\n",
    "for fname in tqdm(fnames):\n",
    "    if atlas == 'aparc':\n",
    "        stc_in_labels = np.load(file=fname, allow_pickle=True)[:-1,:]\n",
    "    if atlas == 'aparc.a2009s':\n",
    "        stc_in_labels = np.load(file=fname, allow_pickle=True)\n",
    "\n",
    "    if normalize == \"z-scoring\":\n",
    "        stc_in_labels = (stc_in_labels - np.mean(stc_in_labels, axis=0)) / np.std(stc_in_labels, axis=0)\n",
    "    \n",
    "    if normalize == \"demean-2norm\":\n",
    "        mean_per_label = np.mean(stc_in_labels, axis=1, keepdims=True)\n",
    "        norm_per_label = np.linalg.norm(stc_in_labels - mean_per_label, ord=2, axis=0)\n",
    "        norm_per_label[norm_per_label == 0] = 1  \n",
    "        stc_in_labels = (stc_in_labels - mean_per_label) / norm_per_label\n",
    "\n",
    "    graph = learn_graph.log_degree_barrier(X=stc_in_labels[:,:end_sample], dist_type='sqeuclidean',\n",
    "                                        alpha=1, beta=1, step=0.5, w0=None, maxit=10000,\n",
    "                                        rtol=1e-16, retall=False, verbosity='NONE')\n",
    "    graph = graph / np.linalg.norm(graph, 'fro') # normalizing it\n",
    "    graphs.append(graph)\n",
    "    \n",
    "graphs = np.array(graphs)\n",
    "avg_graph_control = graphs[:23,:,:].mean(axis=0)\n",
    "avg_graph_case = graphs[23:,:,:].mean(axis=0)\n",
    "\n",
    "# one graph for all case and all control subjects \n",
    "stcs = []\n",
    "for fname in tqdm(fnames):\n",
    "    if atlas == 'aparc':\n",
    "        stc_in_labels = np.load(file=fname, allow_pickle=True)[:-1,:]\n",
    "    if atlas == 'aparc.a2009s':\n",
    "        stc_in_labels = np.load(file=fname, allow_pickle=True)\n",
    "\n",
    "    stcs.append(stc_in_labels[:,:end_sample])\n",
    "\n",
    "x_control = np.concatenate(stcs[:23], axis=1)\n",
    "x_case = np.concatenate(stcs[23:], axis=1)\n",
    "\n",
    "graph_control = learn_graph.log_degree_barrier(X=x_control, dist_type='sqeuclidean',\n",
    "                                        alpha=1, beta=1, step=0.5, w0=None, maxit=10000,\n",
    "                                        rtol=1e-16, retall=False, verbosity='NONE')\n",
    "graph_control = graph_control / np.linalg.norm(graph_control, 'fro') # normalizing it\n",
    "\n",
    "graph_case = learn_graph.log_degree_barrier(X=x_case, dist_type='sqeuclidean',\n",
    "                                        alpha=1, beta=1, step=0.5, w0=None, maxit=10000,\n",
    "                                        rtol=1e-16, retall=False, verbosity='NONE')\n",
    "graph_case = graph_case / np.linalg.norm(graph_case, 'fro') # normalizing it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if atlas == 'aparc':\n",
    "    labels = mne.read_labels_from_annot(subject=\"fsaverage\", parc=atlas,\n",
    "                                        subjects_dir=subjects_dir, verbose=False)[:-1]\n",
    "if atlas == 'aparc.a2009s':\n",
    "    labels = mne.read_labels_from_annot(subject=\"fsaverage\", parc=atlas,\n",
    "                                        subjects_dir=subjects_dir, verbose=False)[:-2]\n",
    "node_coords = []\n",
    "for label in labels:\n",
    "    if label.hemi == 'lh':\n",
    "        hemi = 0\n",
    "    if label.hemi == 'rh':\n",
    "        hemi = 1\n",
    "    center_vertex = label.center_of_mass(subject=\"fsaverage\", \n",
    "                                        restrict_vertices=False, \n",
    "                                        subjects_dir=subjects_dir)\n",
    "    mni_pos = mne.vertex_to_mni(center_vertex, hemis=hemi,\n",
    "                            subject=\"fsaverage\", subjects_dir=subjects_dir)\n",
    "    node_coords.append(mni_pos)\n",
    "\n",
    "node_coords = np.array(node_coords)\n",
    "ticks = [lb.name for lb in labels]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 7))\n",
    "plot_connectome(adjacency_matrix=avg_graph_control, node_coords=node_coords,\n",
    "                node_color='k', node_size=10, axes=axes[0][0],\n",
    "                edge_threshold='93%')\n",
    "\n",
    "plot_connectome(adjacency_matrix=avg_graph_case, node_coords=node_coords,\n",
    "                node_color='k', node_size=10, axes=axes[1][0],\n",
    "                edge_threshold='93%')\n",
    "\n",
    "plot_connectome(adjacency_matrix=graph_control, node_coords=node_coords,\n",
    "                node_color='k', node_size=10, axes=axes[0][1],\n",
    "                edge_threshold='93%')\n",
    "\n",
    "plot_connectome(adjacency_matrix=graph_case, node_coords=node_coords,\n",
    "                node_color='k', node_size=10, axes=axes[1][1],\n",
    "                edge_threshold='93%')\n",
    "\n",
    "axes[0][0].set_ylabel(\"Control\", fontsize=20)\n",
    "axes[1][0].set_ylabel(\"Case\", fontsize=20)\n",
    "axes[0][0].set_title(\"Average of Group\", fontsize=17)\n",
    "axes[0][1].set_title(\"One Graph per Group\", fontsize=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP (both alpha and gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading subjects\n",
    "cmap = plt.get_cmap(\"Set1\")\n",
    "atlas = 'aparc'\n",
    "method = 'mean'\n",
    "dirs = ['/Users/payamsadeghishabestari/meg_gsp/stc_labels_morphed/tinmeg1',\n",
    "        '/Users/payamsadeghishabestari/meg_gsp/stc_labels_morphed/tinmeg3']\n",
    "\n",
    "fnames_alpha = []; fnames_gamma = []\n",
    "for dir in dirs:\n",
    "    for filename in sorted(os.listdir(dir)): \n",
    "        f = os.path.join(dir, filename)\n",
    "        if os.path.isfile(f) and f.endswith(f'{atlas}_{method}.npy'):\n",
    "            if \"alpha\" in f:\n",
    "                fnames_alpha.append(f)\n",
    "            if \"gamma\" in f:\n",
    "                fnames_gamma.append(f)\n",
    "\n",
    "# creating vectors for each time period\n",
    "print(f'Computing the Graph Matrixes ...')\n",
    "sfreq = 250\n",
    "init_time = 2\n",
    "end_time = 300\n",
    "n_frames = 100\n",
    "if atlas == 'aparc':\n",
    "    n_labels = 68\n",
    "if atlas == 'aparc.a2009s':\n",
    "    n_labels = 148\n",
    "up_tri_idxs = np.triu_indices(n_labels, k=1)\n",
    "durations = (np.linspace(init_time, end_time, n_frames) * sfreq).astype(int)\n",
    "all_vectors_alpha = []; all_vectors_gamma = []\n",
    "\n",
    "for fname_alpha, fname_gamma in tzip(fnames_alpha, fnames_gamma):\n",
    "    vectors_alpha = np.zeros(shape=(len(durations), len(up_tri_idxs[0])))\n",
    "    vectors_gamma = np.zeros(shape=(len(durations), len(up_tri_idxs[0])))\n",
    "\n",
    "    stc_in_labels_alpha = np.load(file=fname_alpha, allow_pickle=True)\n",
    "    stc_in_labels_gamma = np.load(file=fname_gamma, allow_pickle=True)\n",
    "    \n",
    "    if atlas == \"aparc\":\n",
    "        stc_in_labels_alpha = stc_in_labels_alpha[:-1,:]\n",
    "        stc_in_labels_gamma = stc_in_labels_gamma[:-1,:]\n",
    "    \n",
    "    for idx, duration in enumerate(durations):\n",
    "        graph_alpha = learn_graph.log_degree_barrier(X=stc_in_labels_alpha[:, :duration],\n",
    "                                                dist_type='sqeuclidean', alpha=1,\n",
    "                                                beta=1, step=0.5, w0=None, maxit=10000,\n",
    "                                                rtol=1e-16, retall=False, verbosity='NONE')\n",
    "        graph_gamma = learn_graph.log_degree_barrier(X=stc_in_labels_gamma[:, :duration],\n",
    "                                                dist_type='sqeuclidean', alpha=1,\n",
    "                                                beta=1, step=0.5, w0=None, maxit=10000,\n",
    "                                                rtol=1e-16, retall=False, verbosity='NONE')\n",
    "        \n",
    "        \n",
    "        vectors_alpha[idx] = graph_alpha[up_tri_idxs]\n",
    "        vectors_gamma[idx] = graph_gamma[up_tri_idxs]\n",
    "    \n",
    "    all_vectors_alpha.append(vectors_alpha)\n",
    "    all_vectors_gamma.append(vectors_gamma)\n",
    "\n",
    "all_vectors_alpha = np.array(all_vectors_alpha)\n",
    "all_vectors_gamma = np.array(all_vectors_gamma)\n",
    "\n",
    "all_vectors = np.concatenate((all_vectors_alpha, all_vectors_gamma), axis=0)\n",
    "\n",
    "# going to umap space\n",
    "print(f'Creating the UMAP space data ...')\n",
    "data_transformed = []\n",
    "for i in tqdm(range(n_frames)):\n",
    "    data = torch.tensor(all_vectors[:,i,:], dtype=torch.float32)\n",
    "    reshaped_data = data.reshape(data.shape[0], -1)\n",
    "    umap_transform = UMAP(n_components=2, n_neighbors=3, \n",
    "                    learning_rate = 1e-7, metric='euclidean')\n",
    "    data_transformed.append(umap_transform.fit_transform(reshaped_data))\n",
    "\n",
    "# creating the animation\n",
    "colors = [cmap.colors[0]] * 23 + [cmap.colors[1]] * 18 + [cmap.colors[2]] * 23 + [cmap.colors[3]] * 18 \n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(-2, 12)\n",
    "ax.set_ylim(-2, 12)\n",
    "ax.grid()\n",
    "ax.set_xlabel('UMAP componet 1')\n",
    "ax.set_ylabel('UMAP componet 2')\n",
    "scatter = ax.scatter([], [], s=25)\n",
    "text = ax.text(-1.5, -1.5, '', fontsize='large', fontstyle='italic')\n",
    "def update(frame):\n",
    "    x = data_transformed[frame][:, 0]\n",
    "    y = data_transformed[frame][:, 1]\n",
    "    offsets = np.column_stack((x, y))\n",
    "    scatter.set_offsets(offsets)\n",
    "    scatter.set_color(colors)\n",
    "    text.set_text(f'Frame {frame}/{n_frames}')\n",
    "    return scatter,\n",
    "\n",
    "animation = FuncAnimation(fig, update, frames=n_frames, init_func=None, repeat=False, interval=300)\n",
    "plt.show()\n",
    "\n",
    "# saving the animation\n",
    "animation.save(f\"/Users/payamsadeghishabestari/meg_gsp/{atlas}_{method}_umap_morphed.gif\", writer=\"pillow\")\n",
    "plt.close(animation._fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate parameters\n",
    "F = 100\n",
    "sfreq = 250\n",
    "end_sample = 5 * 60 * sfreq \n",
    "atlas = 'aparc'\n",
    "freq_range = 'alpha'\n",
    "method = 'mean'\n",
    "subjects_dir = '/Applications/freesurfer/7.4.1/subjects'\n",
    "dirs = ['/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg1',\n",
    "        '/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg3']\n",
    "if atlas == 'aparc':\n",
    "    n_labels = 68\n",
    "if atlas == 'aparc.a2009s':\n",
    "    n_labels = 148\n",
    "\n",
    "# concatenate controls and cases\n",
    "fnames = []\n",
    "for dir in dirs:\n",
    "    for filename in sorted(os.listdir(dir)): \n",
    "        f = os.path.join(dir, filename)\n",
    "        if os.path.isfile(f) and f.endswith(f'{atlas}_{method}.npy') and freq_range in f:\n",
    "            fnames.append(f)\n",
    "stcs = []\n",
    "for fname in fnames:\n",
    "    stc_in_labels = np.load(file=fname, allow_pickle=True)\n",
    "    stcs.append(stc_in_labels[:,:end_sample])\n",
    "x_control = np.concatenate(stcs[:23], axis=1)\n",
    "x_case = np.concatenate(stcs[23:], axis=1)\n",
    "\n",
    "n_surrogates = 100\n",
    "up_tri_idxs = np.triu_indices(n_labels, k=1)\n",
    "vectors_control = np.zeros(shape=(n_surrogates, len(up_tri_idxs[0])))\n",
    "vectors_case = np.zeros(shape=(n_surrogates, len(up_tri_idxs[0])))\n",
    "\n",
    "n_samples = F * sfreq # fixed to 100\n",
    "    \n",
    "# loop over N surrogates\n",
    "for surr_idx in tqdm(range(n_surrogates)):\n",
    "    \n",
    "    # generate random indexes\n",
    "    idxs_control = np.random.randint(1, x_control.shape[1], size=n_samples)\n",
    "    idxs_case = np.random.randint(1, x_case.shape[1], size=n_samples)\n",
    "\n",
    "    # construct graphs per control and case\n",
    "    graph_control = learn_graph.log_degree_barrier(X=x_control[:, idxs_control],\n",
    "                                            dist_type='sqeuclidean', alpha=1,\n",
    "                                            beta=1, step=0.5, w0=None, maxit=10000,\n",
    "                                            rtol=1e-16, retall=False, verbosity='NONE')\n",
    "    graph_case = learn_graph.log_degree_barrier(X=x_case[:, idxs_case],\n",
    "                                            dist_type='sqeuclidean', alpha=1,\n",
    "                                            beta=1, step=0.5, w0=None, maxit=10000,\n",
    "                                            rtol=1e-16, retall=False, verbosity='NONE')\n",
    "    \n",
    "    # normalizing graphs\n",
    "    graph_control = graph_control / np.linalg.norm(graph_control, 'fro') \n",
    "    graph_case = graph_case / np.linalg.norm(graph_case, 'fro')\n",
    "\n",
    "    vectors_control[surr_idx] = graph_control[up_tri_idxs]\n",
    "    vectors_case[surr_idx] = graph_case[up_tri_idxs]\n",
    "    \n",
    "# removing very small connections\n",
    "for i in range(n_surrogates):\n",
    "    zero_edges = np.where(vectors_control[i] < vectors_control[i].max() * 0.01)[0]\n",
    "    vectors_control[i][zero_edges] = 0\n",
    "    zero_edges = np.where(vectors_case[i] < vectors_case[i].max() * 0.01)[0]\n",
    "    vectors_case[i][zero_edges] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics 1\n",
    "(p_thr, alpha) = (0.05, 0.05)  \n",
    "stat, p_values = ttest_ind(vectors_control, vectors_case, permutations=0, random_state=None)\n",
    "my_stat_dict = {}\n",
    "pvals_dict = {}\n",
    "methods = ['bonferroni', 'sidak', 'holm-sidak', 'holm', 'simes-hochberg',\n",
    "            'hommel', 'fdr_bh', 'fdr_by', 'fdr_tsbh', 'fdr_tsbky']\n",
    "for method in methods:\n",
    "    reject_null, p_corrected, _, _ = sm.stats.multipletests(pvals=p_values, alpha=alpha,\n",
    "                                                        method=method)\n",
    "    label_idxs = []\n",
    "    if len(np.where(p_corrected < p_thr)[0]) == 0:\n",
    "        print(f\"No statistical difference between brain labels for method {method}\")\n",
    "    else:\n",
    "        for idx in np.where(p_corrected < p_thr)[0]:\n",
    "            label_idxs.append((up_tri_idxs[0][idx], up_tri_idxs[1][idx]))\n",
    "    my_stat_dict[method] = label_idxs\n",
    "    pvals_dict[method] = p_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics 2\n",
    "(p_thr, alpha) = (0.001, 0.001)  \n",
    "stat, p_values = ttest_ind(vectors_control, vectors_case, permutations=0, alternative=\"two-sided\", random_state=None)\n",
    "# Benjamini/Hochberg correction\n",
    "reject_null, p_corrected, _, _ = sm.stats.multipletests(pvals=p_values, alpha=alpha,\n",
    "                                                        method='bonferroni')\n",
    "label_idxs1 = []\n",
    "label_idxs2 = []\n",
    "k1 = int(len(p_corrected) * 0.01)\n",
    "k2 = int(len(p_corrected) * 0.05)\n",
    "for idx in np.argpartition(p_corrected, k1)[:k1]:\n",
    "    label_idxs1.append((up_tri_idxs[0][idx], up_tri_idxs[1][idx]))\n",
    "for idx in np.argpartition(p_corrected, k2)[:k2]:\n",
    "    label_idxs2.append((up_tri_idxs[0][idx], up_tri_idxs[1][idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization 1\n",
    "atlas = \"aparc\"\n",
    "labels = mne.read_labels_from_annot(subject='fsaverage', parc=atlas, subjects_dir=None, verbose=False)\n",
    "if atlas == 'aparc.a2009s':\n",
    "    labels = labels[:-2]\n",
    "if atlas == 'aparc':\n",
    "    labels = labels[:-1]\n",
    "\n",
    "node_coords = []\n",
    "for label in labels:\n",
    "    if label.hemi == 'lh':\n",
    "        hemi = 0\n",
    "    if label.hemi == 'rh':\n",
    "        hemi = 1\n",
    "    center_vertex = label.center_of_mass(subject='fsaverage', \n",
    "                                        restrict_vertices=False, \n",
    "                                        subjects_dir=None)\n",
    "    mni_pos = mne.vertex_to_mni(center_vertex, hemis=hemi,\n",
    "                            subject='fsaverage', subjects_dir=None)\n",
    "    node_coords.append(mni_pos)\n",
    "\n",
    "node_coords = np.array(node_coords)\n",
    "ticks = [lb.name for lb in labels]\n",
    "\n",
    "n = 5\n",
    "fig, axs = plt.subplots(2, n, figsize=(11, 5))\n",
    "for i, j in zip(range(n), np.random.randint(1, n_surrogates, size=n)):\n",
    "    matrix = np.zeros(shape=(n_labels, n_labels))\n",
    "    matrix[up_tri_idxs] = vectors_control[j]\n",
    "    plot_connectome(adjacency_matrix=matrix + matrix.T, node_coords=node_coords, display_mode=\"z\",\n",
    "                    node_color='k', node_size=20, axes=axs[0][i], colorbar=False,\n",
    "                    edge_threshold=\"95%\")\n",
    "\n",
    "for i, j in zip(range(n), np.random.randint(1, n_surrogates, size=n)):\n",
    "    matrix = np.zeros(shape=(n_labels, n_labels))\n",
    "    matrix[up_tri_idxs] = vectors_case[j]\n",
    "    plot_connectome(adjacency_matrix=matrix + matrix.T, node_coords=node_coords, display_mode=\"z\",\n",
    "                    node_color='k', node_size=20, axes=axs[1][i], colorbar=False,\n",
    "                    edge_threshold=\"95%\")\n",
    "\n",
    "\n",
    "# fig.tight_layout()\n",
    "axs[0][2].set_title(f\"Frame size used: {int(n_samples/sfreq)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization 2\n",
    "control_avg_vector = np.mean(vectors_control, axis=0)\n",
    "control_avg_graph = np.zeros(shape=(n_labels, n_labels))\n",
    "control_avg_graph[up_tri_idxs] = control_avg_vector\n",
    "control_avg_graph = control_avg_graph + control_avg_graph.T\n",
    "\n",
    "\n",
    "case_avg_vector = np.mean(vectors_case, axis=0)\n",
    "case_avg_graph = np.zeros(shape=(n_labels, n_labels))\n",
    "case_avg_graph[up_tri_idxs] = case_avg_vector\n",
    "case_avg_graph = case_avg_graph + case_avg_graph.T\n",
    "\n",
    "diff_graph = control_avg_graph - case_avg_graph\n",
    "\n",
    "labels = mne.read_labels_from_annot(subject='fsaverage', parc=atlas, subjects_dir=None, verbose=False)\n",
    "if atlas == 'aparc.a2009s':\n",
    "    labels = labels[:-2]\n",
    "if atlas == 'aparc':\n",
    "    labels = labels[:-1]\n",
    "\n",
    "node_coords = []\n",
    "for label in labels:\n",
    "    if label.hemi == 'lh':\n",
    "        hemi = 0\n",
    "    if label.hemi == 'rh':\n",
    "        hemi = 1\n",
    "    center_vertex = label.center_of_mass(subject='fsaverage', \n",
    "                                        restrict_vertices=False, \n",
    "                                        subjects_dir=None)\n",
    "    mni_pos = mne.vertex_to_mni(center_vertex, hemis=hemi,\n",
    "                            subject='fsaverage', subjects_dir=None)\n",
    "    node_coords.append(mni_pos)\n",
    "\n",
    "node_coords = np.array(node_coords)\n",
    "ticks = [lb.name for lb in labels]\n",
    "\n",
    "diff_graph = control_avg_graph - case_avg_graph\n",
    "zero_matrix = np.zeros(shape=(n_labels, n_labels))\n",
    "for i, j in label_idxs1:\n",
    "    zero_matrix[i][j] = diff_graph[i][j]\n",
    "stat_graph1 = zero_matrix + zero_matrix.T  \n",
    "\n",
    "zero_matrix = np.zeros(shape=(n_labels, n_labels))\n",
    "for i, j in label_idxs2:\n",
    "    zero_matrix[i][j] = diff_graph[i][j]\n",
    "stat_graph2 = zero_matrix + zero_matrix.T  \n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(11, 5))\n",
    "plot_connectome(adjacency_matrix=stat_graph1, node_coords=node_coords, display_mode=\"lzry\",\n",
    "                node_color='k', node_size=20, axes=axs[0], colorbar=False,\n",
    "                edge_threshold=\"10%\")\n",
    "plot_connectome(adjacency_matrix=stat_graph2, node_coords=node_coords, display_mode=\"lzry\",\n",
    "                node_color='k', node_size=20, axes=axs[1], colorbar=False,\n",
    "                edge_threshold=\"10%\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Phase Amplitude Coupling (run once for all subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/Users/payamsadeghishabestari/meg_gsp/raws/tinmeg3/alpha' \n",
    "subject_ids = []\n",
    "for file in sorted(os.listdir(directory)): ## iterate over folders in that directory\n",
    "    f = os.path.join(directory, file)\n",
    "    if f.endswith(\".fif\"): ## select only folders\n",
    "        subject_ids.append(f[-17:-13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/Users/payamsadeghishabestari/meg_gsp/raws/tinmeg3/alpha' \n",
    "subject_ids = []\n",
    "for file in sorted(os.listdir(directory)): ## iterate over folders in that directory\n",
    "    f = os.path.join(directory, file)\n",
    "    if f.endswith(\".fif\"): ## select only folders\n",
    "        subject_ids.append(f[-17:-13])\n",
    "\n",
    "pacs_dict = {\"ndPAC\": []}\n",
    "idpac = (4, 0, 0)\n",
    "n_labels = 68\n",
    "\n",
    "for subject_id in subject_ids[3:]:\n",
    "    \n",
    "    # read data\n",
    "    fname_alpha = f'/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg3/subject_{subject_id}_alpha_aparc_mean.npy'\n",
    "    fname_gamma = f'/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg3/subject_{subject_id}_gamma_aparc_mean.npy'\n",
    "    stc_alpha = np.load(file=fname_alpha, allow_pickle=True)\n",
    "    stc_gamma = np.load(file=fname_gamma, allow_pickle=True)\n",
    "    \n",
    "    # reshape them\n",
    "    data_pha = np.array([stc_alpha])\n",
    "    data_amp = np.array([stc_gamma])\n",
    "\n",
    "    # compute pac\n",
    "    pacs = np.zeros(shape=(n_labels, n_labels))\n",
    "    for ch_idx1, ch_idx2 in product(range(n_labels), range(n_labels)):\n",
    "        pac_obj = Pac(idpac=idpac, f_pha=[8, 13], f_amp=[30, 80], verbose=None)\n",
    "        pac = pac_obj.filterfit(sf=250, x_pha=data_pha[:,ch_idx1,:], x_amp=data_amp[:,ch_idx2,:], n_perm=200, p=0.05, mcp=\"bonferroni\")\n",
    "        pacs[ch_idx1][ch_idx2] = np.squeeze(pac)\n",
    "    pacs_dict[\"ndPAC\"] = pacs\n",
    "\n",
    "    # save\n",
    "    with open(f'/Users/payamsadeghishabestari/meg_gsp/pacs/alpha-gamma/case/{subject_id}.pkl', 'wb') as file:\n",
    "        pickle.dump(pacs_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make two layer graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = \"539\"\n",
    "fs = 250\n",
    "n_labels = 68\n",
    "tril_idxs = np.tril_indices(2*n_labels, k=-1)\n",
    "\n",
    "# loading subject\n",
    "fname_alpha = f'/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg1/subject_{subject_id}_alpha_aparc_mean.npy'\n",
    "fname_gamma = f'/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg1/subject_{subject_id}_gamma_aparc_mean.npy'\n",
    "pac_file_path = f'/Users/payamsadeghishabestari/meg_gsp/pacs/alpha-gamma/control/{subject_id}.pkl'\n",
    "stc_alpha = np.load(file=fname_alpha, allow_pickle=True)\n",
    "stc_gamma = np.load(file=fname_gamma, allow_pickle=True)\n",
    "with open(pac_file_path, 'rb') as file:\n",
    "    pac_dict = pickle.load(file)\n",
    "\n",
    "# creating the z matrix\n",
    "z = np.zeros(shape=(2 * n_labels, 2 * n_labels))\n",
    "z1 = np.zeros(shape=(n_labels, n_labels))\n",
    "z2 = pac_dict[\"ndPAC\"]\n",
    "z3 = z2.T\n",
    "z4 = np.zeros(shape=(n_labels, n_labels))\n",
    "\n",
    "for idx1, idx2 in product(range(n_labels), range(n_labels)):\n",
    "    z1[idx1][idx2] = coherence(x=stc_alpha[idx1], y=stc_alpha[idx2], fs=fs)[1].mean()\n",
    "    z4[idx1][idx2] = coherence(x=stc_gamma[idx1], y=stc_gamma[idx2], fs=fs)[1].mean()\n",
    "\n",
    "z[:n_labels, :n_labels] = z1\n",
    "z[:n_labels, n_labels:2*n_labels] = z2\n",
    "z[n_labels:2*n_labels, :n_labels] = z3\n",
    "z[n_labels:2*n_labels, n_labels:2*n_labels] = z4\n",
    "z[np.isnan(z)] = 0\n",
    "z = 1 - z[tril_idxs] # 1-coh\n",
    "\n",
    "# graph learning\n",
    "graph = learn_graph.two_layer_log_degree_barrier(z=z, alpha=1, beta=1, step=0.5,\n",
    "                                        w0=None, maxit=10000, rtol=1e-16,\n",
    "                                        retall=False, verbosity='NONE')\n",
    "graph_alpha = graph[:n_labels, :n_labels]\n",
    "graph_cfc = graph[:n_labels, n_labels:2*n_labels]\n",
    "graph_gamma = graph[n_labels:2*n_labels, n_labels:2*n_labels]\n",
    "\n",
    "## remove small connections at each graph separately\n",
    "# thr = 0.01\n",
    "# indices = graph_alpha < graph_alpha.max() * thr\n",
    "# graph_alpha[indices] = 0\n",
    "# indices = graph_cfc < graph_cfc.max() * thr\n",
    "# graph_cfc[indices] = 0\n",
    "# indices = graph_gamma < graph_gamma.max() * thr\n",
    "# graph_gamma[indices] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing\n",
    "graph_type = \"cfc\"\n",
    "subjects_dir = '/Applications/freesurfer/7.4.1/subjects'\n",
    "labels = mne.read_labels_from_annot(subject=\"fsaverage\", parc=\"aparc\",\n",
    "                                    subjects_dir=subjects_dir, verbose=False)[:-1]\n",
    "node_coords = []\n",
    "for label in labels:\n",
    "    if label.hemi == 'lh':\n",
    "        hemi = 0\n",
    "    if label.hemi == 'rh':\n",
    "        hemi = 1\n",
    "    center_vertex = label.center_of_mass(subject=\"fsaverage\", \n",
    "                                        restrict_vertices=False, \n",
    "                                        subjects_dir=None)\n",
    "    mni_pos = mne.vertex_to_mni(center_vertex, hemis=hemi,\n",
    "                            subject=\"fsaverage\", subjects_dir=None)\n",
    "    node_coords.append(mni_pos)\n",
    "\n",
    "node_coords = np.array(node_coords)\n",
    "ticks = [lb.name for lb in labels]\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(11, 5))\n",
    "\n",
    "if graph_type == \"alpha\":\n",
    "    edge_kwargs = None\n",
    "    sub_graph = graph_alpha\n",
    "if graph_type == \"gamma\":\n",
    "    edge_kwargs = None\n",
    "    sub_graph = graph_gamma\n",
    "if graph_type == \"cfc\":\n",
    "    edge_kwargs = {\"lw\": 0.01}\n",
    "    sub_graph = graph_cfc\n",
    "\n",
    "plot_connectome(adjacency_matrix=sub_graph, node_coords=node_coords, display_mode=\"lzry\",\n",
    "                node_color='k', node_size=20, axes=axes, edge_threshold='99.8%', edge_kwargs=edge_kwargs)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/Users/payamsadeghishabestari/meg_gsp/raws/tinmeg1/alpha' \n",
    "subject_ids = []\n",
    "for file in sorted(os.listdir(directory)): ## iterate over folders in that directory\n",
    "    f = os.path.join(directory, file)\n",
    "    if f.endswith(\".fif\"): ## select only folders\n",
    "        subject_ids.append(f[-16:-13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run it once\n",
    "control_subjects_graph = {\"alpha_graphs\" : [], \"gamma_graphs\": [], \"cfc_graphs\": []}\n",
    "fs = 250\n",
    "n_labels = 68\n",
    "tril_idxs = np.tril_indices(2*n_labels, k=-1)\n",
    "\n",
    "for subject_id in tqdm(subject_ids):\n",
    "\n",
    "    # loading subject\n",
    "    fname_alpha = f'/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg1/subject_{subject_id}_alpha_aparc_mean.npy'\n",
    "    fname_gamma = f'/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg1/subject_{subject_id}_gamma_aparc_mean.npy'\n",
    "    pac_file_path = f'/Users/payamsadeghishabestari/meg_gsp/pacs/alpha-gamma/control/{subject_id}.pkl'\n",
    "    stc_alpha = np.load(file=fname_alpha, allow_pickle=True)\n",
    "    stc_gamma = np.load(file=fname_gamma, allow_pickle=True)\n",
    "    with open(pac_file_path, 'rb') as file:\n",
    "        pac_dict = pickle.load(file)\n",
    "\n",
    "    # creating the z matrix\n",
    "    z = np.zeros(shape=(2 * n_labels, 2 * n_labels))\n",
    "    z1 = np.zeros(shape=(n_labels, n_labels))\n",
    "    z2 = pac_dict[\"ndPAC\"]\n",
    "    z3 = z2.T\n",
    "    z4 = np.zeros(shape=(n_labels, n_labels))\n",
    "\n",
    "    for idx1, idx2 in product(range(n_labels), range(n_labels)):\n",
    "        z1[idx1][idx2] = coherence(x=stc_alpha[idx1], y=stc_alpha[idx2], fs=fs)[1].mean()\n",
    "        z4[idx1][idx2] = coherence(x=stc_gamma[idx1], y=stc_gamma[idx2], fs=fs)[1].mean()\n",
    "\n",
    "    z[:n_labels, :n_labels] = z1\n",
    "    z[:n_labels, n_labels:2*n_labels] = z2\n",
    "    z[n_labels:2*n_labels, :n_labels] = z3\n",
    "    z[n_labels:2*n_labels, n_labels:2*n_labels] = z4\n",
    "    \n",
    "\n",
    "    # handling nan values, because there is no signal in some labels\n",
    "    z[np.isnan(z)] = 0\n",
    "    z = 1 - z[tril_idxs] # 1-coh\n",
    "\n",
    "    # graph learning\n",
    "    graph = learn_graph.two_layer_log_degree_barrier(z=z, alpha=1, beta=1, step=0.5,\n",
    "                                            w0=None, maxit=10000, rtol=1e-16,\n",
    "                                            retall=False, verbosity='NONE')\n",
    "    graph_alpha = graph[:n_labels, :n_labels]\n",
    "    graph_cfc = graph[:n_labels, n_labels:2*n_labels]\n",
    "    graph_gamma = graph[n_labels:2*n_labels, n_labels:2*n_labels]\n",
    "\n",
    "    ## remove small connections at each graph separately\n",
    "    # thr = 0.01\n",
    "    # indices = graph_alpha < graph_alpha.max() * thr\n",
    "    # graph_alpha[indices] = 0\n",
    "    # indices = graph_cfc < graph_cfc.max() * thr\n",
    "    # graph_cfc[indices] = 0\n",
    "    # indices = graph_gamma < graph_gamma.max() * thr\n",
    "    # graph_gamma[indices] = 0\n",
    "\n",
    "    ## append to dict\n",
    "    control_subjects_graph[\"alpha_graphs\"].append(graph_alpha)\n",
    "    control_subjects_graph[\"cfc_graphs\"].append(graph_cfc)\n",
    "    control_subjects_graph[\"gamma_graphs\"].append(graph_gamma)\n",
    "\n",
    "# save\n",
    "with open(f'/Users/payamsadeghishabestari/meg_gsp/pacs/two_layer_graph/control_subjects_graph.pkl', 'wb') as file:\n",
    "    pickle.dump(control_subjects_graph, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all brain connectivity\n",
    "key = \"cfc_graphs\"\n",
    "control_file_path = \"/Users/payamsadeghishabestari/meg_gsp/pacs/two_layer_graph/control_subjects_graph.pkl\"\n",
    "case_file_path = \"/Users/payamsadeghishabestari/meg_gsp/pacs/two_layer_graph/case_subjects_graph.pkl\"\n",
    "\n",
    "with open(control_file_path, 'rb') as file:\n",
    "    control_dict = pickle.load(file)\n",
    "with open(case_file_path, 'rb') as file:\n",
    "    case_dict = pickle.load(file)\n",
    "\n",
    "tril_idxs = np.tril_indices(68, k=-1)\n",
    "controls_vector = np.array([i[tril_idxs] for i in np.array(control_dict[key])])\n",
    "case_vector = np.array([i[tril_idxs] for i in np.array(case_dict[key])])\n",
    "\n",
    "\n",
    "(p_thr, alpha) = (0.05, 0.05)  \n",
    "stat, p_values = ttest_ind(controls_vector, case_vector, permutations=0, random_state=None)\n",
    "reject_null, p_corrected, _, _ = sm.stats.multipletests(pvals=p_values, alpha=alpha,\n",
    "                                                        method='fdr_bh')\n",
    "label_idxs = []\n",
    "if len(np.where(p_values < p_thr)[0]) == 0:\n",
    "    print(f\"No statistical difference between brain labels\")\n",
    "else:\n",
    "    for idx in np.where(p_values < p_thr)[0]:\n",
    "        label_idxs.append((tril_idxs[0][idx], tril_idxs[1][idx]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does alpha power decrease in tinnitus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check brain labels\n",
    "directory = '/Users/payamsadeghishabestari/meg_gsp/raws/tinmeg1/alpha' \n",
    "subject_ids = []\n",
    "for file in sorted(os.listdir(directory)): ## iterate over folders in that directory\n",
    "    f = os.path.join(directory, file)\n",
    "    if f.endswith(\".fif\"): ## select only folders\n",
    "        subject_ids.append(f[-16:-13])\n",
    "\n",
    "control_alpha_powers = []\n",
    "for subject_id in subject_ids:\n",
    "    fname_alpha = f'/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg1/subject_{subject_id}_alpha_aparc_mean.npy'\n",
    "    stc_alpha = np.load(file=fname_alpha, allow_pickle=True)\n",
    "    control_alpha_powers.append(np.mean(np.square(stc_alpha), axis=1))\n",
    "\n",
    "directory = '/Users/payamsadeghishabestari/meg_gsp/raws/tinmeg3/alpha' \n",
    "subject_ids = []\n",
    "for file in sorted(os.listdir(directory)): ## iterate over folders in that directory\n",
    "    f = os.path.join(directory, file)\n",
    "    if f.endswith(\".fif\"): ## select only folders\n",
    "        subject_ids.append(f[-17:-13])\n",
    "\n",
    "case_alpha_powers = []\n",
    "for subject_id in subject_ids:\n",
    "    fname_alpha = f'/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg3/subject_{subject_id}_alpha_aparc_mean.npy'\n",
    "    stc_alpha = np.load(file=fname_alpha, allow_pickle=True)\n",
    "    case_alpha_powers.append(np.mean(np.square(stc_alpha), axis=1))\n",
    "\n",
    "labels = mne.read_labels_from_annot(subject=\"fsaverage\", parc=\"aparc\",\n",
    "                                    subjects_dir=None, verbose=False)[:-1]\n",
    "ticks = [lb.name for lb in labels]\n",
    "(p_thr, alpha) = (0.05, 0.05)  \n",
    "stat, p_values = ttest_ind(np.array(control_alpha_powers), np.array(case_alpha_powers),\n",
    "                            permutations=0, random_state=None)\n",
    "reject_null, p_corrected, _, _ = sm.stats.multipletests(pvals=p_values, alpha=alpha,\n",
    "                                                        method='fdr_bh')\n",
    "\n",
    "if len(np.where(p_corrected < p_thr)[0]) == 0:\n",
    "    print(f\"No statistical difference\")\n",
    "else:\n",
    "    for idx in np.where(p_corrected < p_thr)[0]:\n",
    "        print(f\"difference at brain label: {ticks[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check topomaps\n",
    "directory = '/Users/payamsadeghishabestari/meg_gsp/raws/tinmeg1/alpha' \n",
    "subject_ids = []\n",
    "for file in sorted(os.listdir(directory)): ## iterate over folders in that directory\n",
    "    f = os.path.join(directory, file)\n",
    "    if f.endswith(\".fif\"): ## select only folders\n",
    "        subject_ids.append(f[-16:-13])\n",
    "\n",
    "spectra_control = []\n",
    "for subject_id in tqdm(subject_ids):\n",
    "    raw = mne.io.read_raw_fif(fname=f\"/Users/payamsadeghishabestari/meg_gsp/raws/tinmeg1/alpha/{subject_id}_raw_tsss.fif\", preload=True, verbose=False)\n",
    "    spectra_control.append(raw.compute_psd(fmin=0.1, fmax=40, verbose=False))\n",
    "\n",
    "\n",
    "directory = '/Users/payamsadeghishabestari/meg_gsp/raws/tinmeg3/alpha' \n",
    "subject_ids = []\n",
    "for file in sorted(os.listdir(directory)): ## iterate over folders in that directory\n",
    "    f = os.path.join(directory, file)\n",
    "    if f.endswith(\".fif\"): ## select only folders\n",
    "        subject_ids.append(f[-17:-13])\n",
    "\n",
    "spectra_case = []\n",
    "for subject_id in tqdm(subject_ids):\n",
    "    raw = mne.io.read_raw_fif(fname=f\"/Users/payamsadeghishabestari/meg_gsp/raws/tinmeg3/alpha/{subject_id}_raw_tsss.fif\", preload=True, verbose=False)\n",
    "    spectra_case.append(raw.compute_psd(fmin=0.1, fmax=40, verbose=False))\n",
    "\n",
    "total_control = [i.get_data() for i in spectra_control]\n",
    "data_control = np.array(total_control).mean(axis=0)\n",
    "total_case = [i.get_data() for i in spectra_case]\n",
    "data_case = np.array(total_case).mean(axis=0)\n",
    "data_diff = data_control - data_case\n",
    "\n",
    "info = spectra_control[0].info\n",
    "freqs = spectra_control[0].freqs\n",
    "grand_spectrum_control = mne.time_frequency.SpectrumArray(data_control, info, freqs)\n",
    "grand_spectrum_case = mne.time_frequency.SpectrumArray(data_case, info, freqs)\n",
    "diff_spectrum = mne.time_frequency.SpectrumArray(data_diff, info, freqs)\n",
    "\n",
    "bands = {'Alpha (8-13 Hz)': (8, 13)}\n",
    "diff_spectrum.plot_topomap(bands=bands, vlim=(0, 1000))\n",
    "\n",
    "(p_thr, alpha) = (0.05, 0.05)  \n",
    "stat, p_values = ttest_ind(np.array(total_control).mean(axis=2), np.array(total_case).mean(axis=2),\n",
    "                            permutations=0, random_state=None)\n",
    "reject_null, p_corrected, _, _ = sm.stats.multipletests(pvals=p_values, alpha=alpha,\n",
    "                                                        method='fdr_bh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_control_norm = np.array([i / np.array(total_control).mean(axis=0) for i in np.array(total_control)])\n",
    "total_case_norm = np.array([i / np.array(total_case).mean(axis=0) for i in np.array(total_case)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p_values = ttest_ind(np.array(total_control_norm).mean(axis=2), np.array(total_case_norm).mean(axis=2),\n",
    "                            permutations=0, random_state=None)\n",
    "reject_null, p_corrected, _, _ = sm.stats.multipletests(pvals=p_values, alpha=alpha,\n",
    "                                                        method='fdr_bh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K sample testing via independent tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyppo.ksample import KSample\n",
    "import pandas as pd\n",
    "import warnings\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## subjects to select\n",
    "co_include = range(23)\n",
    "ca_include = range(18)\n",
    "method = \"fdr_bh\"\n",
    "key = \"cfc_graphs\"\n",
    "\n",
    "## load all subjects\n",
    "control_file_path = \"/Users/payamsadeghishabestari/meg_gsp/pacs/two_layer_graph/control_subjects_graph.pkl\"\n",
    "case_file_path = \"/Users/payamsadeghishabestari/meg_gsp/pacs/two_layer_graph/case_subjects_graph.pkl\"\n",
    "\n",
    "with open(control_file_path, 'rb') as file:\n",
    "    control_dict = pickle.load(file)\n",
    "with open(case_file_path, 'rb') as file:\n",
    "    case_dict = pickle.load(file)\n",
    "\n",
    "samples_co = np.array(control_dict[key])[co_include]\n",
    "samples_ca = np.array(case_dict[key])[ca_include]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## subjects to select\n",
    "co_include = range(23)\n",
    "ca_include = range(18)\n",
    "method = \"fdr_bh\"\n",
    "key = \"cfc_graphs\"\n",
    "\n",
    "## load all subjects\n",
    "control_file_path = \"/Users/payamsadeghishabestari/meg_gsp/pacs/two_layer_graph/control_subjects_graph.pkl\"\n",
    "case_file_path = \"/Users/payamsadeghishabestari/meg_gsp/pacs/two_layer_graph/case_subjects_graph.pkl\"\n",
    "\n",
    "with open(control_file_path, 'rb') as file:\n",
    "    control_dict = pickle.load(file)\n",
    "with open(case_file_path, 'rb') as file:\n",
    "    case_dict = pickle.load(file)\n",
    "\n",
    "samples_co = np.array(control_dict[key])[co_include]\n",
    "samples_ca = np.array(case_dict[key])[ca_include]\n",
    "\n",
    "connectomes = [samples_co, samples_ca]\n",
    "\n",
    "indices = zip(*np.triu_indices(68, 1))\n",
    "edge_pvals = []\n",
    "for roi_i, roi_j in indices:\n",
    "\n",
    "    # Get the (i,j)-th edge for each connectome\n",
    "    samples = [type[:, roi_i, roi_j] for type in connectomes]\n",
    "\n",
    "    # Calculate the p-value for the (i,j)-th edge\n",
    "    try:\n",
    "        statistic, pvalue = KSample(\"Dcorr\").test(*samples, reps=10000, workers=-1)\n",
    "    except ValueError:\n",
    "        # A ValueError is thrown when any of the samples have equal edge\n",
    "        # weights (i.e. one of the inputs has 0 variance)\n",
    "        statistic = np.nan\n",
    "        pvalue = 1\n",
    "\n",
    "    edge_pvals.append([roi_i, roi_j, statistic, pvalue])\n",
    "\n",
    "# Convert the nested list to a dataframe\n",
    "edge_pvals_df = pd.DataFrame(edge_pvals, columns=[\"ROI_1\", \"ROI_2\", \"stat\", \"pval\"])\n",
    "\n",
    "# multiple comparison\n",
    "alpha = 0.05\n",
    "ps = [item[3] for item in edge_pvals]\n",
    "reject_null, corr_ps, _, _ = sm.stats.multipletests(pvals=ps, alpha=alpha,\n",
    "                                                        method=method)\n",
    "for p_idx, p in enumerate(corr_ps):\n",
    "    edge_pvals[p_idx][3] = p\n",
    "\n",
    "edge_pvals_df[\"corr_pval\"] = corr_ps\n",
    "edge_pvals_df[\"significant\"] = (edge_pvals_df[\"corr_pval\"] < alpha)\n",
    "\n",
    "# Get the top 10 strongest signal edges\n",
    "edge_pvals_df.sort_values(by=\"corr_pval\", inplace=True, ignore_index=True)\n",
    "edge_pvals_top = edge_pvals_df.head(10)\n",
    "\n",
    "# Replace ROI indices with actual names\n",
    "labels = mne.read_labels_from_annot(subject=\"fsaverage\", parc=\"aparc\",\n",
    "                                    subjects_dir=None, verbose=False)[:-1]\n",
    "ticks = [lb.name for lb in labels]\n",
    "def lookup_roi_name(index):\n",
    "    roi_name = ticks[index]\n",
    "    return f\"{roi_name}\"\n",
    "\n",
    "edge_pvals_top[\"ROI_1\"] = edge_pvals_top[\"ROI_1\"].apply(lookup_roi_name)\n",
    "edge_pvals_top[\"ROI_2\"] = edge_pvals_top[\"ROI_2\"].apply(lookup_roi_name)\n",
    "edge_pvals_top.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import networkx as nx\n",
    "from graspologic.match import graph_match\n",
    "from graspologic.utils import import_graph\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random indexes (sampling)\n",
    "atlas = 'aparc'\n",
    "freq_range = 'alpha'\n",
    "method = 'mean'\n",
    "sfreq = 250\n",
    "n_labels = 68\n",
    "init_time = 2\n",
    "end_time = 300\n",
    "up_tri_idxs = np.triu_indices(n_labels, k=1)\n",
    "dirs = ['/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg1',\n",
    "        '/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg3']\n",
    "fnames = []\n",
    "for dir in dirs:\n",
    "    for filename in sorted(os.listdir(dir)): \n",
    "        f = os.path.join(dir, filename)\n",
    "        if os.path.isfile(f) and f.endswith(f'{atlas}_{method}.npy') and freq_range in f:\n",
    "            fnames.append(f)\n",
    "\n",
    "n_frames = 30\n",
    "random_arrays = []\n",
    "for n_sec in np.linspace(init_time, end_time, n_frames):\n",
    "    random_array = random.sample(range(end_time*sfreq), k=int(n_sec*sfreq))\n",
    "    random_arrays.append(random_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "fname = \"/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg1/subject_539_gamma_aparc_mean.npy\"\n",
    "times = np.linspace(init_time, end_time, n_frames)\n",
    "graph_df = {\"subject_id\": [], \"frequency_range\": [], \"alpha\": [], \"beta\": [], \"modularity\": [], \n",
    "            \"avg_deg\": [], \"avg_cc\": [], \"coverage\": [],\n",
    "            \"performance\": [], \"density\": [], \"euc_dist\": [],\n",
    "            \"matching_score\": [], \"times_used\": []}\n",
    "\n",
    "stc_in_labels = np.load(file=fname, allow_pickle=True)\n",
    "params = [0.1, 0.25, 0.5, 1, 1.5, 2, 2.5]\n",
    "\n",
    "\n",
    "for idx, random_array in tqdm(enumerate(random_arrays)):    \n",
    "\n",
    "    for alpha, beta in product(params, params): # loop over parameters    \n",
    "        \n",
    "        # learn graph\n",
    "        graph = learn_graph.log_degree_barrier(X=stc_in_labels[:, random_array],\n",
    "                                                dist_type='sqeuclidean', alpha=alpha,\n",
    "                                                beta=beta, step=0.5, w0=None, maxit=10000,\n",
    "                                                rtol=1e-16, retall=False, verbosity='NONE')\n",
    "        # normalizing it\n",
    "        graph = graph / np.linalg.norm(graph, 'fro')\n",
    "        # drop small edges\n",
    "        thr = 0.01 * np.max(graph)\n",
    "        graph[graph < thr] = 0\n",
    "        \n",
    "        # compute preferences\n",
    "        G = nx.from_numpy_matrix(np.array(graph))\n",
    "        part = nx.community.greedy_modularity_communities(G)\n",
    "        \n",
    "        # fill the dataframe\n",
    "        graph_df[\"modularity\"].append(nx.community.modularity(G, part))\n",
    "        graph_df[\"avg_deg\"].append(np.mean([d for n, d in G.degree()]))\n",
    "        graph_df[\"avg_cc\"].append(nx.average_clustering(G))\n",
    "        graph_df[\"coverage\"].append(nx.community.partition_quality(G, part)[0])\n",
    "        graph_df[\"performance\"].append(nx.community.partition_quality(G, part)[1])\n",
    "        graph_df[\"density\"].append(nx.density(G))\n",
    "        # graph_df[\"avg_short_path\"].append(nx.average_shortest_path_length(G))\n",
    "\n",
    "        # compute best graph\n",
    "        graph_last = learn_graph.log_degree_barrier(X=stc_in_labels[:, :],\n",
    "                                                dist_type='sqeuclidean', alpha=alpha,\n",
    "                                                beta=beta, step=0.5, w0=None, maxit=10000,\n",
    "                                                rtol=1e-16, retall=False, verbosity='NONE')\n",
    "        # normalizing it\n",
    "        graph_last = graph_last / np.linalg.norm(graph_last, 'fro')\n",
    "        # drop small edges\n",
    "        thr = 0.01 * np.max(graph_last)\n",
    "        graph_last[graph_last < thr] = 0\n",
    "\n",
    "        # compute euclidean distance\n",
    "        graph_df[\"euc_dist\"].append(distance.euclidean(graph.flatten(), graph_last.flatten()))\n",
    "\n",
    "        # graph matching\n",
    "        G = import_graph(G)\n",
    "        G_last = nx.from_numpy_matrix(np.array(graph_last))\n",
    "        G_last = import_graph(G_last)\n",
    "        graph_df[\"matching_score\"].append(graph_match(G, G_last)[2])\n",
    "\n",
    "        # fill the parameters\n",
    "        graph_df[\"alpha\"].append(alpha)\n",
    "        graph_df[\"beta\"].append(beta)\n",
    "        graph_df[\"times_used\"].append(times[idx])\n",
    "        graph_df[\"subject_id\"].append(fname[-24:-21])\n",
    "        graph_df[\"frequency_range\"].append(\"gamma\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame(graph_df)\n",
    "df.to_csv(\"/Users/payamsadeghishabestari/meg_gsp/graph_matching/df_10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and plot the dataframes\n",
    "dfs_list = [pd.read_csv(f\"/Users/payamsadeghishabestari/meg_gsp/graph_matching/df_{i}.csv\") for i in range(1, 11)]\n",
    "df = pd.concat(dfs_list)\n",
    "\n",
    "mask1 = df[\"alpha\"].isin([0.1, 0.25, 0.5, 1, 1.5, 2, 2.5])\n",
    "df = df[mask1]\n",
    "mask2 = df[\"beta\"].isin([0.5, 1, 1.5])\n",
    "df = df[mask2]\n",
    "df_alpha = df[df[\"frequency_range\"]==\"alpha\"]\n",
    "df_gamma = df[df[\"frequency_range\"]==\"gamma\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"markersize\": 3, \"lw\": 2, \"ax\": ax}\n",
    "g = sns.catplot(data=df_alpha, x=\"times_used\", y=\"matching_score\", hue=\"beta\", col=\"alpha\", # row=\"frequency_range\",\n",
    "    capsize=.2, palette=\"YlGnBu_d\", errorbar=\"se\",\n",
    "    kind=\"point\", markers=[\"o\", \"*\", \"s\"], height=6, aspect=.75, **kwargs)\n",
    "g.set_xticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing networks with Classical correlation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne_connectivity import spectral_connectivity_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters to adjust\n",
    "statistics = \"t-test\" # \"Mann-Whitney U test\"\n",
    "F = 300\n",
    "sfreq = 250\n",
    "alpha = 0.05  # significance level\n",
    "p_thr = 0.05\n",
    "end_sample = F * sfreq\n",
    "\n",
    "# loading subjects\n",
    "atlas = 'aparc'\n",
    "freq_ranges = [\"delta\", \"theta\", \"alpha\", \"beta\", \"gamma\"]\n",
    "method = 'mean'\n",
    "dirs = ['/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg1',\n",
    "        '/Users/payamsadeghishabestari/meg_gsp/stc_labels/tinmeg3']\n",
    "\n",
    "labels = mne.read_labels_from_annot(subject='fsaverage', parc=atlas, subjects_dir=None, verbose=False)[:-1]\n",
    "lb_names = np.array([lb.name for lb in labels])\n",
    "\n",
    "stats_dict = {}\n",
    "pvals_dict = {}\n",
    "\n",
    "freqs = np.linspace(30, 80, 6)\n",
    "for freq_range in freq_ranges[4:]:    \n",
    "    fnames = []\n",
    "    for dir in dirs:\n",
    "        for filename in sorted(os.listdir(dir)): \n",
    "            f = os.path.join(dir, filename)\n",
    "            if os.path.isfile(f) and f.endswith(f'{atlas}_{method}.npy') and freq_range in f:\n",
    "                fnames.append(f)\n",
    "\n",
    "# creating co\n",
    "print(f'Computing the Graph Matrixes ...')\n",
    "if atlas == 'aparc':\n",
    "    n_labels = 68\n",
    "if atlas == 'aparc.a2009s':\n",
    "    n_labels = 148\n",
    "up_tri_idxs = np.triu_indices(n_labels, k=1)\n",
    "vectors = []\n",
    "\n",
    "for fname in tqdm(fnames):\n",
    "    stc_in_labels = np.load(file=fname, allow_pickle=True)\n",
    "    data = np.array([stc_in_labels])\n",
    "\n",
    "    con = spectral_connectivity_time(data=data, freqs=freqs, method=\"wpli\",\n",
    "                                average=True, sfreq=sfreq, faverage=True,\n",
    "                                verbose=False)\n",
    "    con_data = con.get_data(output=\"dense\")[:,:,0].T\n",
    "    \n",
    "    vector = con_data[up_tri_idxs]\n",
    "    vectors.append(vector)\n",
    "\n",
    "vectors = np.array(vectors)\n",
    "\n",
    "# removing very small connections\n",
    "# for i in range(len(fnames)):\n",
    "#     zero_edges = np.where(vectors[i] < vectors[i].max() * 0.01)[0]\n",
    "#     vectors[i][zero_edges] = 0\n",
    "\n",
    "# statistics\n",
    "control_group = vectors[:23,:]\n",
    "case_group = vectors[23:,:]\n",
    "\n",
    "control_group[np.where(np.isnan(control_group))] = 0\n",
    "case_group[np.where(np.isnan(case_group))] = 0\n",
    "\n",
    "if statistics == 't-test':\n",
    "    stat, p_values = ttest_ind(control_group, case_group,\n",
    "                                permutations=0, random_state=42)\n",
    "if statistics == 'Mann-Whitney U test':\n",
    "    stat, p_values = mannwhitneyu(control_group, case_group)\n",
    "\n",
    "my_stat_dict = {}\n",
    "p_dict = {}\n",
    "methods = ['bonferroni', 'fdr_bh']\n",
    "for method in methods:\n",
    "    reject_null, p_corrected, _, _ = sm.stats.multipletests(pvals=p_values, alpha=alpha,\n",
    "                                                        method=method)\n",
    "    label_idxs = []\n",
    "    if len(np.where(p_corrected < p_thr)[0]) == 0:\n",
    "        print(f\"No statistical difference between brain labels for method {method}\")\n",
    "    else:\n",
    "        for idx in np.where(p_corrected < p_thr)[0]:\n",
    "            label_idxs.append((up_tri_idxs[0][idx], up_tri_idxs[1][idx]))\n",
    "    my_stat_dict[method] = lb_names[label_idxs]\n",
    "    p_dict[method] = p_corrected\n",
    "\n",
    "stats_dict[freq_range] = my_stat_dict\n",
    "pvals_dict[freq_range] = p_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the labels thickness / volume / area and graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"thickness\"\n",
    "hemisphere = \"lh\"\n",
    "mc_method = 'bonferroni' # 'fdr_bh'\n",
    "labels = mne.read_labels_from_annot(subject='fsaverage', parc=\"aparc\", subjects_dir=None, verbose=False)[:-1]\n",
    "lb_names = np.array([lb.name for lb in labels])\n",
    "\n",
    "dfs_list = []\n",
    "for hemi in [\"lh\", \"rh\"]:\n",
    "    fname = f\"/Users/payamsadeghishabestari/KI_MEG/structural_stats/aparc_stats_{mode}_{hemi}.csv\"\n",
    "    df = pd.read_csv(fname, sep=\"\\s+\")\n",
    "    new_column_names = [col[3:] for col in df.columns]\n",
    "    df.columns = new_column_names\n",
    "    df[\"hemi\"] = hemi\n",
    "    group_ids = [\"control\"] * 26 + [\"case\"] * 18\n",
    "    df[\"group\"] = group_ids\n",
    "    dfs_list.append(df)\n",
    "\n",
    "df = pd.concat(dfs_list)\n",
    "df = df.drop(columns=['inSegVolNotVent', 'V'])\n",
    "\n",
    "if mode == \"thickness\":\n",
    "    df = df.drop(columns=\"MeanThickness_thickness\")\n",
    "if mode == \"area\":\n",
    "    df = df.drop(columns=\"WhiteSurfArea_area\")   \n",
    "\n",
    "mask = df[f\"aparc.{mode}\"].isin([697, 750, 853]) # drop these subjects\n",
    "df = df[~mask]\n",
    "df = df.reset_index().drop(columns=\"index\")\n",
    "df_hemi = df[df[\"hemi\"]==f'{hemisphere}']\n",
    "df_control = df_hemi.query('group==\"control\"')\n",
    "df_case = df_hemi.query('group==\"case\"')\n",
    "\n",
    "##\n",
    "my_dict = {\"label_name\": [],\n",
    "            \"hemisphere\": [],\n",
    "            f\"average_{mode}_control\": [],\n",
    "            f\"average_{mode}_case\": [],\n",
    "            \"p_value\": [],\n",
    "            \"p_corrected\": [],\n",
    "            \"significant\": []}\n",
    "\n",
    "p_values = []\n",
    "for col in list(df_hemi.columns[1:-2]):\n",
    "    stat, p_value = ttest_ind(df_control[col], df_case[col],\n",
    "                                permutations=0, random_state=42)\n",
    "    p_values.append(p_value)\n",
    "\n",
    "    my_dict[\"label_name\"].append(col)\n",
    "    my_dict[f\"average_{mode}_control\"].append(df_control[col].mean())\n",
    "    my_dict[f\"average_{mode}_case\"].append(df_case[col].mean())\n",
    "    my_dict[\"p_value\"].append(p_value)\n",
    "\n",
    "## after multiple comparison\n",
    "reject_null, p_corrected, _, _ = sm.stats.multipletests(pvals=p_values, alpha=0.05,\n",
    "                                                        method=mc_method)\n",
    "\n",
    "my_dict[\"p_corrected\"] = p_corrected\n",
    "my_dict[\"significant\"] = reject_null\n",
    "my_dict[\"hemisphere\"] = hemisphere\n",
    "df_stat = pd.DataFrame(my_dict)\n",
    "df_stat.sort_values(by=\"p_corrected\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting the significant ones\n",
    "y = \"superiorparietal_thickness\"\n",
    "palette_color = ['#1f77b4', '#d62728']\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "sns.violinplot(data=df, x=\"group\", y=y, hue=\"hemi\",\n",
    "                palette=palette_color, fill=False, ax=ax, inner=None, legend=False)\n",
    "sns.swarmplot(data=df, x=\"group\", y=y, hue=\"hemi\", dodge=True,\n",
    "                palette=palette_color, ax=ax)\n",
    "ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"aparc.thickness\", inplace=True)\n",
    "\n",
    "## construct a graph for control\n",
    "df_sub = df[df[\"group\"]==f\"control\"]\n",
    "corr_matrix_control = np.zeros(shape=(len(lb_names), len(lb_names)))\n",
    "\n",
    "for lb1_idx, lb1 in enumerate(lb_names):\n",
    "    for lb2_idx, lb2 in enumerate(lb_names):\n",
    "        (hemi1, hemi2) = (lb1[-2:], lb2[-2:])\n",
    "\n",
    "        df_sub1 = df_sub[df_sub[\"hemi\"]==f\"{hemi1}\"]\n",
    "        df_sub2 = df_sub[df_sub[\"hemi\"]==f\"{hemi2}\"]\n",
    "\n",
    "        x = df_sub1[f\"{lb1[:-3]}_thickness\"]\n",
    "        y = df_sub2[f\"{lb2[:-3]}_thickness\"]\n",
    "        corr_matrix_control[lb1_idx][lb2_idx] = np.corrcoef(x.values, y.values)[0][1]\n",
    "\n",
    "## construct a graph for case\n",
    "df_sub = df[df[\"group\"]==f\"case\"]\n",
    "corr_matrix_case = np.zeros(shape=(len(lb_names), len(lb_names)))\n",
    "\n",
    "for lb1_idx, lb1 in enumerate(lb_names):\n",
    "    for lb2_idx, lb2 in enumerate(lb_names):\n",
    "        (hemi1, hemi2) = (lb1[-2:], lb2[-2:])\n",
    "\n",
    "        df_sub1 = df_sub[df_sub[\"hemi\"]==f\"{hemi1}\"]\n",
    "        df_sub2 = df_sub[df_sub[\"hemi\"]==f\"{hemi2}\"]\n",
    "\n",
    "        x = df_sub1[f\"{lb1[:-3]}_thickness\"]\n",
    "        y = df_sub2[f\"{lb2[:-3]}_thickness\"]\n",
    "        corr_matrix_case[lb1_idx][lb2_idx] = np.corrcoef(x.values, y.values)[0][1]\n",
    "\n",
    "## conver 1 to 0\n",
    "for i, j in product(range(68), range(68)):\n",
    "    if i == j:\n",
    "        corr_matrix_control[i][j] = 0\n",
    "        corr_matrix_case[i][j] = 0\n",
    "\n",
    "## remove small connections\n",
    "thr = 0.01 * corr_matrix_control.max()\n",
    "corr_matrix_control[abs(corr_matrix_control) < thr] = 0\n",
    "\n",
    "thr = 0.01 * corr_matrix_case.max()\n",
    "corr_matrix_case[abs(corr_matrix_case) < thr] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_dir = '/Applications/freesurfer/7.4.1/subjects'\n",
    "labels = mne.read_labels_from_annot(subject=\"fsaverage\", parc=\"aparc\",\n",
    "                                    subjects_dir=subjects_dir, verbose=False)[:-1]\n",
    "node_coords = []\n",
    "for label in labels:\n",
    "    if label.hemi == 'lh':\n",
    "        hemi = 0\n",
    "    if label.hemi == 'rh':\n",
    "        hemi = 1\n",
    "    center_vertex = label.center_of_mass(subject=\"fsaverage\", \n",
    "                                        restrict_vertices=False, \n",
    "                                        subjects_dir=None)\n",
    "    mni_pos = mne.vertex_to_mni(center_vertex, hemis=hemi,\n",
    "                            subject=\"fsaverage\", subjects_dir=None)\n",
    "    node_coords.append(mni_pos)\n",
    "\n",
    "node_coords = np.array(node_coords)\n",
    "ticks = [lb.name for lb in labels]\n",
    "edge_kwargs = None\n",
    "\n",
    "diff_corr = corr_matrix_case - corr_matrix_control\n",
    "fig, axes = plt.subplots(1, 1, figsize=(11, 5))\n",
    "\n",
    "plot_connectome(adjacency_matrix=diff_corr, node_coords=node_coords, display_mode=\"lzry\",\n",
    "                node_color='k', node_size=20, axes=axes, edge_threshold='99.7%', edge_kwargs=edge_kwargs)\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
